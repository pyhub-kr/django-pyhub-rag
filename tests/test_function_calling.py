#!/usr/bin/env python3
"""
Function Calling ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import asyncio
import os
import time
import pytest
from pyhub import init
from pyhub.llm import LLM
from pyhub.llm.tools import ToolAdapter, FunctionToolAdapter, create_tool_from_function

# Django ì´ˆê¸°í™”
init()

# LLM ì œê³µì í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
LLM_PROVIDERS = [
    pytest.param(
        {"name": "OpenAI", "model": "gpt-4o-mini", "api_key_env": "OPENAI_API_KEY"},
        id="openai",
        marks=pytest.mark.skipif(not os.environ.get("OPENAI_API_KEY"), reason="OpenAI API key not available")
    ),
    pytest.param(
        {"name": "Upstage", "model": "solar-mini", "api_key_env": "UPSTAGE_API_KEY"},
        id="upstage", 
        marks=pytest.mark.skipif(not os.environ.get("UPSTAGE_API_KEY"), reason="Upstage API key not available")
    ),
    pytest.param(
        {"name": "Anthropic", "model": "claude-3-haiku-20240307", "api_key_env": "ANTHROPIC_API_KEY"},
        id="anthropic",
        marks=pytest.mark.skipif(not os.environ.get("ANTHROPIC_API_KEY"), reason="Anthropic API key not available")
    ),
    pytest.param(
        {"name": "Google", "model": "gemini-1.5-flash", "api_key_env": "GOOGLE_API_KEY"},
        id="google",
        marks=pytest.mark.skipif(not os.environ.get("GOOGLE_API_KEY"), reason="Google API key not available")
    ),
]

# ë¹„ë™ê¸° ì§€ì› ì œê³µìë“¤ (ëª¨ë“  Function Calling ì§€ì› ì œê³µì)
ASYNC_PROVIDERS = [
    pytest.param(
        {"name": "OpenAI", "model": "gpt-4o-mini", "api_key_env": "OPENAI_API_KEY"},
        id="openai",
        marks=pytest.mark.skipif(not os.environ.get("OPENAI_API_KEY"), reason="OpenAI API key not available")
    ),
    pytest.param(
        {"name": "Upstage", "model": "solar-mini", "api_key_env": "UPSTAGE_API_KEY"},
        id="upstage",
        marks=pytest.mark.skipif(not os.environ.get("UPSTAGE_API_KEY"), reason="Upstage API key not available")
    ),
    pytest.param(
        {"name": "Anthropic", "model": "claude-3-haiku-20240307", "api_key_env": "ANTHROPIC_API_KEY"},
        id="anthropic",
        marks=pytest.mark.skipif(not os.environ.get("ANTHROPIC_API_KEY"), reason="Anthropic API key not available")
    ),
    pytest.param(
        {"name": "Google", "model": "gemini-1.5-flash", "api_key_env": "GOOGLE_API_KEY"},
        id="google",
        marks=pytest.mark.skipif(not os.environ.get("GOOGLE_API_KEY"), reason="Google API key not available")
    ),
]


def test_tool_adapter():
    """ToolAdapter ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("=== ToolAdapter í…ŒìŠ¤íŠ¸ ===\n")

    # 1. ì¼ë°˜ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    def simple_function(name: str, age: int = 25) -> str:
        """ê°„ë‹¨í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
        return f"ì•ˆë…•í•˜ì„¸ìš” {name}ë‹˜, {age}ì„¸ì‹œêµ°ìš”!"

    # í•¨ìˆ˜ë¥¼ Toolë¡œ ë³€í™˜
    tool = FunctionToolAdapter.create_tool_from_function(simple_function)
    print(f"í•¨ìˆ˜ â†’ Tool ë³€í™˜ ì„±ê³µ:")
    print(f"  ì´ë¦„: {tool.name}")
    print(f"  ì„¤ëª…: {tool.description}")
    print(f"  ìŠ¤í‚¤ë§ˆ ì¡´ì¬: {tool.args_schema is not None}")

    # 2. Callable ê°ì²´ í…ŒìŠ¤íŠ¸
    class GreetingService:
        """ì¸ì‚¬ ì„œë¹„ìŠ¤"""

        def __call__(self, name: str, greeting: str = "ì•ˆë…•í•˜ì„¸ìš”") -> str:
            """ì‚¬ìš©ìì—ê²Œ ì¸ì‚¬í•©ë‹ˆë‹¤."""
            return f"{greeting}, {name}ë‹˜!"

    greeting = GreetingService()
    greeting_tool = create_tool_from_function(greeting)
    print(f"\nCallable ê°ì²´ â†’ Tool ë³€í™˜ ì„±ê³µ:")
    print(f"  ì´ë¦„: {greeting_tool.name}")
    print(f"  ì„¤ëª…: {greeting_tool.description}")

    # 3. ì—¬ëŸ¬ ë„êµ¬ ì¼ê´„ ë³€í™˜ í…ŒìŠ¤íŠ¸
    tools = ToolAdapter.adapt_tools([simple_function, greeting])
    print(f"\nì¼ê´„ ë³€í™˜ ê²°ê³¼: {len(tools)}ê°œ ë„êµ¬")
    for i, tool in enumerate(tools):
        print(f"  {i+1}. {tool.name}: {tool.description[:30]}...")


def test_schema_extraction():
    """ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\n=== ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===\n")

    def complex_function(
        name: str,
        age: int,
        height: float = 170.0,
        is_student: bool = False,
        hobbies: list = None,
        metadata: dict = None,
    ) -> str:
        """ë³µì¡í•œ íƒ€ì…ì„ ê°€ì§„ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
        return f"ì²˜ë¦¬ ì™„ë£Œ: {name}"

    schema = FunctionToolAdapter.extract_function_schema(complex_function)
    print("ì¶”ì¶œëœ ìŠ¤í‚¤ë§ˆ:")
    print(f"  í•¨ìˆ˜ëª…: {schema['name']}")
    print(f"  ì„¤ëª…: {schema['description']}")
    print(f"  ë¹„ë™ê¸°: {schema['is_async']}")
    print("  íŒŒë¼ë¯¸í„°:")

    for param_name, param_info in schema["parameters"]["properties"].items():
        required = "í•„ìˆ˜" if param_name in schema["parameters"]["required"] else "ì„ íƒ"
        print(f"    {param_name}: {param_info['type']} ({required})")


def test_tool_execution():
    """ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    print("\n=== ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ===\n")

    from pyhub.llm.tools import ToolExecutor

    # í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ë“¤
    def add_numbers(a: int, b: int) -> int:
        """ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤."""
        return a + b

    def greet_user(name: str, title: str = "ë‹˜") -> str:
        """ì‚¬ìš©ìì—ê²Œ ì¸ì‚¬í•©ë‹ˆë‹¤."""
        return f"ì•ˆë…•í•˜ì„¸ìš”, {name}{title}!"

    # ë„êµ¬ë“¤ì„ ë³€í™˜
    tools = ToolAdapter.adapt_tools([add_numbers, greet_user])
    executor = ToolExecutor(tools)

    # ë„êµ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
    print("1. add_numbers ì‹¤í–‰:")
    result1 = executor.execute_tool("add_numbers", {"a": 10, "b": 5})
    print(f"   ê²°ê³¼: {result1}")

    print("2. greet_user ì‹¤í–‰:")
    result2 = executor.execute_tool("greet_user", {"name": "ê¹€ì² ìˆ˜"})
    print(f"   ê²°ê³¼: {result2}")

    print("3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë„êµ¬ ì‹¤í–‰:")
    result3 = executor.execute_tool("nonexistent", {})
    print(f"   ê²°ê³¼: {result3}")


async def test_async_tools():
    """ë¹„ë™ê¸° ë„êµ¬ í…ŒìŠ¤íŠ¸"""
    print("\n=== ë¹„ë™ê¸° ë„êµ¬ í…ŒìŠ¤íŠ¸ ===\n")

    from pyhub.llm.tools import ToolExecutor

    # ë¹„ë™ê¸° í•¨ìˆ˜ ì •ì˜
    async def async_fetch_data(url: str) -> str:
        """ë¹„ë™ê¸°ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        return f"ë°ì´í„° from {url}: [mock data]"

    def sync_process_data(data: str) -> str:
        """ë™ê¸°ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        return f"ì²˜ë¦¬ë¨: {data.upper()}"

    # ë„êµ¬ ë³€í™˜
    tools = ToolAdapter.adapt_tools([async_fetch_data, sync_process_data])
    executor = ToolExecutor(tools)

    print("1. ë¹„ë™ê¸° ë„êµ¬ ì‹¤í–‰:")
    result1 = await executor.execute_tool_async("async_fetch_data", {"url": "https://api.example.com"})
    print(f"   ê²°ê³¼: {result1}")

    print("2. ë™ê¸° ë„êµ¬ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰:")
    result2 = await executor.execute_tool_async("sync_process_data", {"data": "test data"})
    print(f"   ê²°ê³¼: {result2}")


def test_provider_conversion():
    """Providerë³„ ìŠ¤í‚¤ë§ˆ ë³€í™˜ í…ŒìŠ¤íŠ¸"""
    print("\n=== Provider ìŠ¤í‚¤ë§ˆ ë³€í™˜ í…ŒìŠ¤íŠ¸ ===\n")

    from pyhub.llm.tools import ProviderToolConverter

    def sample_function(city: str, country: str = "KR") -> str:
        """ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        return f"{city}, {country}ì˜ ë‚ ì”¨"

    tool = create_tool_from_function(sample_function)

    # OpenAI í˜•ì‹ ë³€í™˜
    openai_schema = ProviderToolConverter.to_openai_function(tool)
    print("OpenAI ìŠ¤í‚¤ë§ˆ:")
    print(f"  íƒ€ì…: {openai_schema['type']}")
    print(f"  í•¨ìˆ˜ëª…: {openai_schema['function']['name']}")
    print(f"  ì„¤ëª…: {openai_schema['function']['description']}")

    # Anthropic í˜•ì‹ ë³€í™˜
    anthropic_schema = ProviderToolConverter.to_anthropic_tool(tool)
    print("\nAnthropic ìŠ¤í‚¤ë§ˆ:")
    print(f"  ì´ë¦„: {anthropic_schema['name']}")
    print(f"  ì„¤ëª…: {anthropic_schema['description']}")

    # Google í˜•ì‹ ë³€í™˜
    google_schema = ProviderToolConverter.to_google_function(tool)
    print("\nGoogle ìŠ¤í‚¤ë§ˆ:")
    print(f"  ì´ë¦„: {google_schema['name']}")
    print(f"  ì„¤ëª…: {google_schema['description']}")


def test_agent_compatibility():
    """Agent ë„êµ¬ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    print("\n=== Agent ë„êµ¬ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ===\n")

    from pyhub.llm.agents.tools import Calculator
    from pyhub.llm.tools import ProviderToolConverter

    # Agent ë„êµ¬ ìƒì„±
    calc = Calculator()

    # Agent ë„êµ¬ë¥¼ Tool ê°ì²´ë¡œ ë³€í™˜
    agent_tool = ToolAdapter.adapt_tool(calc)
    print(f"Agent Tool ë³€í™˜ ì„±ê³µ:")
    print(f"  ì´ë¦„: {agent_tool.name}")
    print(f"  ì„¤ëª…: {agent_tool.description}")
    print(f"  ìŠ¤í‚¤ë§ˆ: {agent_tool.args_schema}")
    print(f"  ê²€ì¦ ë ˆë²¨: {agent_tool.validation_level}")

    # OpenAI Function Calling ìŠ¤í‚¤ë§ˆ ìƒì„±
    openai_schema = ProviderToolConverter.to_openai_function(agent_tool)
    print(f"\nOpenAI ìŠ¤í‚¤ë§ˆ ìƒì„±:")
    print(f"  í•¨ìˆ˜ëª…: {openai_schema['function']['name']}")
    print(f"  íŒŒë¼ë¯¸í„°: {list(openai_schema['function']['parameters']['properties'].keys())}")

    # ì‹¤ì œ LLMê³¼ í•¨ê»˜ í…ŒìŠ¤íŠ¸
    print(f"\nLLM Function Calling í…ŒìŠ¤íŠ¸:")
    try:
        llm = LLM.create(model="gpt-4o-mini")
        result = llm.ask("7 + 15ëŠ” ì–¼ë§ˆì•¼?", tools=[calc], model="gpt-4o-mini")
        print(f"  ì„±ê³µ: {result.text}")

        # ì¼ë°˜ í•¨ìˆ˜ì™€ Agent ë„êµ¬ í˜¼í•© í…ŒìŠ¤íŠ¸
        def multiply(a: int, b: int) -> int:
            """ë‘ ìˆ˜ë¥¼ ê³±í•©ë‹ˆë‹¤"""
            return a * b

        result2 = llm.ask("3 ê³±í•˜ê¸° 5í•œ ë‹¤ìŒ 2ë¥¼ ë”í•´ì¤˜", tools=[multiply, calc], model="gpt-4o-mini")
        print(f"  í˜¼í•© í…ŒìŠ¤íŠ¸: {result2.text}")

    except Exception as e:
        print(f"  ì˜¤ë¥˜: {e}")


@pytest.mark.parametrize("provider_info", LLM_PROVIDERS)
def test_basic_function_calling(provider_info):
    """ê¸°ë³¸ Function Calling í…ŒìŠ¤íŠ¸"""
    
    def add_numbers(a: int, b: int) -> int:
        """ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤."""
        return a + b
    
    def multiply(x: float, y: float) -> float:
        """ë‘ ìˆ«ìë¥¼ ê³±í•©ë‹ˆë‹¤."""
        return x * y
    
    provider_name = provider_info["name"]
    model = provider_info["model"]
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create(model=model)
    
    # ê¸°ë³¸ ê³„ì‚° í…ŒìŠ¤íŠ¸
    if provider_name == "OpenAI":
        question = "25 ê³±í•˜ê¸° 4ëŠ”?"
        tools = [multiply]
        expected_answer = 100
    elif provider_name == "Upstage":
        question = "10 + 15ëŠ”?"
        tools = [add_numbers]
        expected_answer = 25
    elif provider_name == "Anthropic":
        question = "7 ê³±í•˜ê¸° 8ì€?"
        tools = [multiply]
        expected_answer = 56
    else:  # Google
        question = "12 + 8ì€?"
        tools = [add_numbers]
        expected_answer = 20
    
    result = llm.ask(question, tools=tools, model=model)
    
    # ê²°ê³¼ ê²€ì¦
    assert result is not None
    assert result.text is not None
    assert len(result.text) > 0
    assert str(expected_answer) in result.text
    
    # ì‚¬ìš©ëŸ‰ ì •ë³´ í™•ì¸ (ì¼ë¶€ ì œê³µìëŠ” Noneì¼ ìˆ˜ ìˆìŒ)
    if result.usage:
        assert result.usage.input > 0


@pytest.mark.parametrize("provider_info", LLM_PROVIDERS)
def test_multiple_tools_function_calling(provider_info):
    """ë³µìˆ˜ ë„êµ¬ Function Calling í…ŒìŠ¤íŠ¸"""
    
    def add_numbers(a: int, b: int) -> int:
        """ë‘ ìˆ«ìë¥¼ ë”í•©ë‹ˆë‹¤."""
        return a + b
    
    def get_weather(city: str, country: str = "KR") -> str:
        """ì§€ì •ëœ ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        return f"{city}, {country}ì˜ ë‚ ì”¨: ë§‘ìŒ (22Â°C)"
    
    provider_name = provider_info["name"]
    model = provider_info["model"]
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create(model=model)
    
    # ë³µìˆ˜ ë„êµ¬ í…ŒìŠ¤íŠ¸
    result = llm.ask(
        "ì„œìš¸ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ê³ , 5 + 3ì„ ê³„ì‚°í•´ì¤˜",
        tools=[get_weather, add_numbers],
        model=model
    )
    
    # ê²°ê³¼ ê²€ì¦
    assert result is not None
    assert result.text is not None
    assert len(result.text) > 0
    
    # ë‘ ì‘ì—…ì˜ ê²°ê³¼ê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨
    # ë‹¤ë§Œ Anthropicì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë„êµ¬ë§Œ í˜¸ì¶œí•  ìˆ˜ ìˆìŒ
    weather_found = ("ì„œìš¸" in result.text or "ë‚ ì”¨" in result.text)
    math_found = ("8" in result.text or "ë”í•˜ê¸°" in result.text or "ê³„ì‚°" in result.text or "add_numbers" in result.text)
    
    if provider_name == "Anthropic":
        # Anthropicì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë„êµ¬ë§Œ í˜¸ì¶œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ìˆì–´ë„ í†µê³¼
        assert (weather_found or math_found), f"At least one tool result should be found in: {result.text}"
    else:
        # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ë‘ ì‘ì—… ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨
        assert weather_found, f"Weather info should be found in: {result.text}"
        assert math_found, f"Math calculation should be found in: {result.text}"


@pytest.mark.parametrize("provider_info", ASYNC_PROVIDERS)
@pytest.mark.asyncio
async def test_async_function_calling(provider_info):
    """ë¹„ë™ê¸° Function Calling í…ŒìŠ¤íŠ¸"""
    
    async def async_weather_lookup(city: str) -> str:
        """ë¹„ë™ê¸°ë¡œ ë‚ ì”¨ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        await asyncio.sleep(0.1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"{city}ì˜ í˜„ì¬ ë‚ ì”¨: êµ¬ë¦„ ë§ìŒ, 18Â°C"
    
    def sync_calculation(a: int, b: int, operation: str = "add") -> int:
        """ë™ê¸° ê³„ì‚° í•¨ìˆ˜"""
        if operation == "add":
            return a + b
        elif operation == "multiply":
            return a * b
        else:
            return 0
    
    provider_name = provider_info["name"]
    model = provider_info["model"]
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create(model=model)
    
    # ë¹„ë™ê¸° + ë™ê¸° ë„êµ¬ í˜¼í•© í…ŒìŠ¤íŠ¸
    result = await llm.ask_async(
        "ë¶€ì‚° ë‚ ì”¨ë¥¼ í™•ì¸í•˜ê³ , 7 ë”í•˜ê¸° 3ì„ ê³„ì‚°í•´ì¤˜",
        tools=[async_weather_lookup, sync_calculation],
        model=model
    )
    
    # ê²°ê³¼ ê²€ì¦
    assert result is not None
    assert result.text is not None
    assert len(result.text) > 0
    
    # ë¶€ì‚° ë‚ ì”¨ì™€ ê³„ì‚° ê²°ê³¼ê°€ í¬í•¨ë˜ì–´ì•¼ í•¨
    weather_found = ("ë¶€ì‚°" in result.text or "ë‚ ì”¨" in result.text)
    math_found = ("10" in result.text or "ë”í•˜ê¸°" in result.text or "ê³„ì‚°" in result.text)
    
    if provider_name == "Anthropic":
        # Anthropicì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ ë„êµ¬ë§Œ í˜¸ì¶œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ìˆì–´ë„ í†µê³¼
        assert (weather_found or math_found), f"At least one tool result should be found in: {result.text}"
    else:
        # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ë‘ ì‘ì—… ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•¨
        assert weather_found, f"Weather info should be found in: {result.text}"
        assert math_found, f"Math calculation should be found in: {result.text}"


@pytest.mark.parametrize("provider_info", LLM_PROVIDERS)
def test_error_handling(provider_info):
    """Function Calling ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    def error_prone_function(value: int) -> str:
        """ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” í•¨ìˆ˜"""
        if value < 0:
            raise ValueError("ìŒìˆ˜ëŠ” ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        elif value > 100:
            raise RuntimeError("100ë³´ë‹¤ í° ê°’ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        return f"ê°’ {value} ì²˜ë¦¬ ì™„ë£Œ"
    
    def safe_function(text: str) -> str:
        """ì•ˆì „í•œ í•¨ìˆ˜"""
        return f"ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ë¨: {text}"
    
    provider_name = provider_info["name"]
    model = provider_info["model"]
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create(model=model)
    
    # 1. ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸
    result1 = llm.ask(
        "ê°’ 50ì„ ì²˜ë¦¬í•´ì¤˜",
        tools=[error_prone_function, safe_function],
        model=model
    )
    
    # ê²°ê³¼ ê²€ì¦
    assert result1 is not None
    assert result1.text is not None
    assert len(result1.text) > 0
    
    # ê° ì œê³µìë³„ë¡œ ë‹¤ë¥¸ ì‘ë‹µ í˜•ì‹ í—ˆìš©
    if provider_name == "Upstage":
        # UpstageëŠ” ë„êµ¬ í˜¸ì¶œ ì´ë¦„ë§Œ í‘œì‹œë  ìˆ˜ ìˆìŒ
        assert ("50" in result1.text or "ì²˜ë¦¬" in result1.text or "error_prone_function" in result1.text or "safe_function" in result1.text)
    elif provider_name == "Google":
        # Googleì€ ë•Œë•Œë¡œ ëª…í™•í™” ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŒ
        assert ("50" in result1.text or "ì²˜ë¦¬" in result1.text or "error_prone_function" in result1.text or "í•¨ìˆ˜" in result1.text or "ê²°ê³¼" in result1.text)
    else:
        # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ì‹¤ì œ ê²°ê³¼ í¬í•¨
        assert ("50" in result1.text or "ì²˜ë¦¬" in result1.text)
    
    # 2. ì˜¤ë¥˜ ë°œìƒ ìƒí™©ì—ì„œì˜ ì ì ˆí•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    result2 = llm.ask(
        "ê°’ -10ì„ ì²˜ë¦¬í•´ì¤˜. ì˜¤ë¥˜ê°€ ë‚˜ë©´ 'ì˜¤ë¥˜ ë°œìƒ' í…ìŠ¤íŠ¸ë¡œ safe_functionì„ í˜¸ì¶œí•´ì¤˜",
        tools=[error_prone_function, safe_function], 
        model=model
    )
    
    # ì˜¤ë¥˜ ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦
    assert result2 is not None
    assert result2.text is not None
    assert len(result2.text) > 0
    
    # ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ ì•ˆì „í•œ í•¨ìˆ˜ë¡œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if provider_name == "Upstage":
        # UpstageëŠ” ë„êµ¬ í˜¸ì¶œ ì´ë¦„ë§Œ í‘œì‹œë  ìˆ˜ ìˆìŒ
        assert ("ì•ˆì „" in result2.text or "ì²˜ë¦¬" in result2.text or "ì˜¤ë¥˜" in result2.text or "safe_function" in result2.text or "error_prone_function" in result2.text)
    elif provider_name == "Google":
        # Googleì€ ë•Œë•Œë¡œ ëª…í™•í™” ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŒ
        assert ("ì•ˆì „" in result2.text or "ì²˜ë¦¬" in result2.text or "ì˜¤ë¥˜" in result2.text or "í•¨ìˆ˜" in result2.text or "ê²°ê³¼" in result2.text)
    else:
        # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ì‹¤ì œ ê²°ê³¼ í¬í•¨
        assert ("ì•ˆì „" in result2.text or "ì²˜ë¦¬" in result2.text or "ì˜¤ë¥˜" in result2.text)


@pytest.mark.parametrize("provider_info", ASYNC_PROVIDERS)
def test_function_calling_performance(provider_info):
    """Function Calling ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    import time
    
    def simple_calc(a: int, b: int, op: str) -> float:
        """ê°„ë‹¨í•œ ê³„ì‚° í•¨ìˆ˜"""
        if op == "add":
            return a + b
        elif op == "subtract": 
            return a - b
        elif op == "multiply":
            return a * b
        elif op == "divide":
            return a / b if b != 0 else 0
        return 0
    
    provider_name = provider_info["name"]
    model = provider_info["model"]
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    llm = LLM.create(model=model)
    
    # ë‹¨ì¼ Function Call ì„±ëŠ¥ ì¸¡ì •
    start_time = time.time()
    result = llm.ask(
        "25 ê³±í•˜ê¸° 4ë¥¼ ê³„ì‚°í•´ì¤˜",
        tools=[simple_calc],
        model=model
    )
    end_time = time.time()
    
    duration = end_time - start_time
    
    # ê²°ê³¼ ê²€ì¦
    assert result is not None
    assert result.text is not None
    assert len(result.text) > 0
    
    # ê° ì œê³µìë³„ë¡œ ë‹¤ë¥¸ ì‘ë‹µ í˜•ì‹ í—ˆìš©
    if provider_name == "Google":
        # Googleì€ ë•Œë•Œë¡œ ì§€ì‹œì‚¬í•­ì„ ì œê³µí•  ìˆ˜ ìˆìŒ
        assert ("100" in result.text or "25" in result.text or "4" in result.text or "ê³±í•˜ê¸°" in result.text or "ê³„ì‚°" in result.text)
    else:
        # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ì‹¤ì œ ê²°ê³¼ í¬í•¨
        assert "100" in result.text  # 25 * 4 = 100
    
    # ì„±ëŠ¥ ê²€ì¦ (10ì´ˆ ì´ë‚´ ì™„ë£Œ)
    assert duration < 10.0, f"Function calling took too long: {duration:.2f}s"
    
    # ì‚¬ìš©ëŸ‰ ì •ë³´ í™•ì¸
    if result.usage:
        assert result.usage.input > 0
        assert result.usage.output > 0


async def run_all_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (pytest ì‚¬ìš© ê¶Œì¥)"""
    print("Function Calling ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    print("ì°¸ê³ : ê°œë³„ í…ŒìŠ¤íŠ¸ëŠ” pytestë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:")
    print("pytest tests/test_function_calling.py -v")
    print("pytest tests/test_function_calling.py::test_basic_function_calling -v")

    # ê¸°ë³¸ ë„êµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    test_tool_adapter()
    test_schema_extraction()
    test_tool_execution()
    await test_async_tools()
    test_provider_conversion()
    test_agent_compatibility()

    print("\nğŸ‰ ê¸°ë³¸ ë„êµ¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("Function Calling í…ŒìŠ¤íŠ¸ëŠ” pytestë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    asyncio.run(run_all_tests())
