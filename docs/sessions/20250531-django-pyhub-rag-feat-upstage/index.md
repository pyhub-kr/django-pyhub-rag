# ì¥á„€á…© á„€á…µá„‡á…¡á†«á„‹á…³á„…á…© PDF á„‘á…¡á„‰á…µá†¼á„‡á…®á„á…¥ á„†á…®á†«á„‰á…¥ á„€á…µá„‡á…¡á†« RAG á„á…¡á„Œá…µ (2025á„‚á…§á†« 5á„‹á…¯á†¯ 31á„‹á…µá†¯)

<div class="video-container">
<iframe src="https://www.youtube.com/embed/1aQH5x49Nic?si=dtgtDRfXAefOO7V2" allowfullscreen></iframe>
</div>

+ [PDF ìŠ¬ë¼ì´ë“œ ë‹¤ìš´ë¡œë“œ](./slide.pdf)

!!! TODO

    ë‚´ìš©ì„ ì•„ë˜ì— ì •ë¦¬ ì¤‘.

---

## ë‹¤ë£° ë‚´ìš©

+ django-pyhub-rag ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì—…ìŠ¤í…Œì´ì§€ Document Parse API í†µí•©
    - PDF íŒŒì‹± + ì´ë¯¸ì§€ ì¶”ì¶œ, ì´ë¯¸ì§€ ì„¤ëª… ì‘ì„±ê¹Œì§€ 1ë²ˆì˜ ëª…ë ¹ìœ¼ë¡œ ìˆ˜í–‰
+ ì¥ê³  ëª¨ë¸ ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ëŠ” ì „ì²´ ê³¼ì •(ì¥ê³  í”„ë¡œì íŠ¸ ìƒì„±ë¶€í„°)ì„ ë¼ì´ë¸Œ ì‹œì—°

## RAG

> ê²€ìƒ‰ ì¦ê°• ìƒì„± (Retrieval Augmented Generation)

## ë¬¸ì„œ ë³€í™˜

> ë¬¸ì„œ ë ˆì´ì•„ì›ƒì„ ì‚´ë ¤ Plain Textë¡œ ë³€í™˜

### ì—…ìŠ¤í…Œì´ì§€ API ìš”ì²­ ì˜ˆì‹œ

> ì—…ìŠ¤í…Œì´ì§€ Document Parse API í™œìš© ($0.01 / í˜ì´ì§€)

+ ìƒ˜í”Œ ì½”ë“œ : [https://console.upstage.ai/docs/capabilities/document-digitization/document-parsing#example](https://console.upstage.ai/docs/capabilities/document-digitization/document-parsing#example)
+ ìƒ˜í”Œ íŒŒì¼ : [https://github.com/pyhub-kr/django-pyhub-rag/tree/main/samples](https://github.com/pyhub-kr/django-pyhub-rag/tree/main/samples)

``` python
import json
import requests
import os

# API Key ì§€ì • : https://console.upstage.ai/api-keys?api=document-parsing
API_KEY = os.getenv("UPSTAGE_API_KEY")

filepath = "./samples/á„’á…§á†«á„ƒá…¢ á„Šá…¡á†«á„á…¡á„‘á…¦ 2025 Hyundai Owner's Manual.pdf"  # ì…ë ¥ íŒŒì¼ : 743 í˜ì´ì§€ ($7.43)

url = "https://api.upstage.ai/v1/document-digitization"
headers = {"Authorization": f"Bearer {API_KEY}"}

files = {"document": open(filepath, "rb")}
data = {
    "ocr": "force",
    "model": "document-parse",
    "align_orientation": True,              # ë¬¸ì„œ ë°©í–¥ ìë™ ì¡°ì •
    "coordinates": True,                    # ìœ„ì¹˜ ì¸ì‹
    "output_formats": "['markdown']",       # or 'html', 'text'
    # ì§€ì • ë ˆì´ì•„ì›ƒ íƒ€ì…ì˜ ì»¨í…ì¸ ë¥¼ base64 ì´ë¯¸ì§€ë¡œì„œ ë°›ê¸°
    # https://console.upstage.ai/docs/capabilities/document-digitization/document-parsing#understanding-the-outputs
    "base64_encoding": "['chart', 'equation', 'figure', 'table']",
}
response = requests.post(url, headers=headers, files=files, data=data)

# API ì‘ë‹µì„ íŒŒì¼ì— ì“°ê¸°
with open("./samples/á„’á…§á†«á„ƒá…¢ á„Šá…¡á†«á„á…¡á„‘á…¦ 2025 Hyundai Owner's Manual.json", "wt", encoding="utf-8") as f:
    json.dump(response.json(), f, indent=4, ensure_ascii=False)
```

### django-pyhub-rag í™œìš©

``` shell
pip install --upgrade "django-pyhub-rag[all]"

# ë³€í™˜ ëª…ë ¹ : UPSTAGE_API_KEY, OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ í•„ìš”
python -m pyhub.parser upstage --enable-image-descriptor íŒŒì¼ê²½ë¡œ.pdf
python -m pyhub.parser upstage -i íŒŒì¼ê²½ë¡œ.pdf
# API ìºì‹œ í™œì„±í™” : íŒŒì¼ ì‹œìŠ¤í…œ ìºì‹œ, ìµœëŒ€ 5000ê°œ, íƒ€ì„ì•„ì›ƒ 3ì¼
python -m pyhub.parser upstage --enable-cache -i íŒŒì¼ê²½ë¡œ.pdf
```

### ì´ë¯¸ì§€ ì„¤ëª… ìŠ¤í¬ë¦½íŠ¸, ì»¤ìŠ¤í…€ ì§€ì›

``` shell title="pyhub mcptools ì„¤ì •íŒŒì¼ ìƒì„±, ê²½ë¡œ, ì¶œë ¥, ìˆ˜ì •"
python -m pyhub toml create # ì„¤ì •íŒŒì¼ ìƒì„±
python -m pyhub toml path   # ì„¤ì •íŒŒì¼ ê²½ë¡œ ì¶œë ¥
python -m pyhub toml show   # ì„¤ì •íŒŒì¼ ë‚´ìš© í‘œì¤€ì¶œë ¥
python -m pyhub toml edit   # ì„¤ì •íŒŒì¼ í¸ì§‘
```

### ë­ì²´ì¸, UpstageDocumentParseParser

ë™ê¸° ë°©ì‹ë§Œ ì§€ì›

+ [GitHub ì €ì¥ì†Œ](https://github.com/langchain-ai/langchain-upstage/blob/main/libs/upstage/langchain_upstage/document_parse_parsers.py#L169)
+ ë°°ì¹˜í¬ê¸° ì˜µì…˜
    - API í˜¸ì¶œ ì‹œ, ì—¬ëŸ¬ í˜ì´ì§€ ì§€ì› ì—¬ë¶€
    - ë°°ì¹˜ ì—¬ë¶€ ì§€ì • (1í˜ì´ì§€ or 10í˜ì´ì§€)
+ ë¬¸ì„œ ë¨¸ì§€ ì „ëµ
    - page, element, none

### django-pyhub-rag, UpstageDocumentParseParser

ë™ê¸°/ë¹„ë™ê¸° ë°©ì‹ ì§€ì›

+ ë°°ì¹˜í¬ê¸° ì˜µì…˜
    - API í˜¸ì¶œ ì‹œ, ì—¬ëŸ¬ í˜ì´ì§€ ì§€ì› ì—¬ë¶€
    - ë°°ì¹˜ í¬ê¸° ì§ì ‘ ì§€ì • ì§€ì›
+ ë¬¸ì„œ ë¨¸ì§€ ì „ëµ
    - page, element, none
+ ë‹¤ì–‘í•œ í˜ì´ì§€ ë²ˆí˜¸ ì§€ì • ì§€ì›
+ LLMí†µí•œ ì´ë¯¸ì§€ ì„¤ëª… ì§€ì›
+ API ìºì‹± ì§€ì› (ì¥ê³  ìºì‹± ê¸°ë°˜)
    - ë™ì¼ í˜ì´ì§€ì— ëŒ€í•´ì„œëŠ” API ìš”ì²­ Xâ€¨(ë°°ì¹˜ í¬ê¸°=1 ì§€ì • í•„ìš”)
+ ë¹„ë™ê¸° ì§€ì›
+ ì¥ê³  File API ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ì§€ì›
    - AWS S3, Azure Storage ë“± ìƒì˜ PDF ì½ê¸° ê°€ëŠ¥

``` python title="src/pyhub/parser/upstage/parser.py"

from django.core.files import File

class UpstageDocumentParseParser:
    # ...

    async def _call_document_parse_api(self, files: dict, timeout: int = DEFAULT_TIMEOUT) -> dict:
        headers = {
            "Authorization": f"Bearer {self.upstage_api_key}",
        }
        data = {
            "ocr": self.ocr_mode,
            "model": self.model,
            "output_formats": "['html', 'text', 'markdown']",
            "coordinates": self.coordinates,
            "base64_encoding": "[" + ",".join(f"'{el}'" for el in self.base64_encoding_category_list) + "]",
        }

        response_data: bytes = await cached_http_async(
            self.api_url,
            method="POST",
            headers=headers,
            data=data,
            files=files,
            timeout=timeout,
            ignore_cache=self.ignore_cache,
            cache_alias="upstage",
        )
        return json.loads(response_data)
```

## pyhub.llm í™œìš© ì˜ˆì‹œ

### LLM ì‘ë‹µ ìƒì„±

> OpenAI, Anthropic, Google, Upstage, Ollamaì„ ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ë¡œ ì§€ì›

``` python
from pyhub.llm import OpenAILLM
# UpstageLLM, AnthropicLLM, GoogleLLM, OllamaLLM

llm = OpenAILLM(
    # model="gpt-4o-mini",
    # temperature=0.2,
    # max_tokens=1000,
    # api_key=  # default: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜
)

reply = llm.ask("hello")
print(reply)
print(reply.usage)
```

### LLM.create

> ì§€ì› ëª¨ë¸ëª… íƒ€ì…ìœ¼ë¡œ ìë™ ì™„ì„± ì§€ì›

``` python
from pyhub.llm import LLM

# ì—¬ëŸ¬ LLM APIë¥¼ í™œìš©í•´ì•¼í•  ë•Œ, modelëª… ì§€ì • ë§Œìœ¼ë¡œ ì†ì‰½ê²Œ ìŠ¤ìœ„ì¹­
llm = LLM.create(model="gemini-1.5-flash")
# llm = GoogleLLM(model="gemini-1.5-flash") ê³¼ ë™ì¼

reply = llm.ask("hello")
print(reply)
print(reply.usage)
```

### stream ì§€ì›

> Generator[Reply] ë°˜í™˜. ë§ˆì§€ë§‰ chunk ì‘ë‹µì— usage

``` python
for reply in llm.ask("ëŒ€ì „ ì—¬í–‰ì§€ 3ê³³ ì•Œë ¤ì¤˜", stream=True)
    print(reply.text, end="", flush=True)
print()
print(reply.usage)
```

### ì„ íƒ ì§€ì›

> OpenAI/Upstage APIì—ì„œëŠ” Structured Output í™œìš©í•˜ì—¬, ë³´ë‹¤ ê²¬ê³ í•œ ì„ íƒ ì§€ì›

``` python
from pyhub.llm import OpenAILLM

llm = OpenAILLM()

reply = llm.ask("ê³¼ì¼ì„ ê³¨ë¼ì¤˜.", choices=["apple", "car"])
print(repr(reply))
print(reply.choice)        # Optional[str] : ì„ íƒí•  ìˆ˜ ì—†ë‹¤ë©´ None
print(reply.choice_index)  # Optional[int] : ì„ íƒí•  ìˆ˜ ì—†ë‹¤ë©´ None
print(reply.usage)
```

ì¶œë ¥

```
Reply(text='{"choice": "apple", "confidence": 0.85}', choice='apple', choice_index=0, confidence=0.85)
apple
0
Usage(input=65, output=11)
```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì§€ì›

> ìºì‹±, í…œí”Œë¦¿, ëª¨ë¸ ë“±ì˜ ì¥ê³  ê¸°ëŠ¥ì„ í™œìš©í•  ë•Œì—ëŠ” pyhub.init() í˜¸ì¶œ í•„ìš”

``` python
from pyhub.llm import OpenAILLM

llm = OpenAILLM(system_prompt="ë„ˆëŠ” ìœ ëŠ¥í•œ ì—¬í–‰ ê°€ì´ë“œ",
                prompt="{ì—¬í–‰ì§€}ì˜ ë§›ì§‘ì„ 10ê°œ ì†Œê°œí•´ì¤˜")
reply = llm.ask({"ì—¬í–‰ì§€": "ëŒ€ì „"})
print(reply.text[:100], "...")
print(reply.usage)
```

``` python
from pyhub import init
from pyhub.llm import OpenAILLM

init()

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì§€ì›
llm = OpenAILLM(
    system_prompt="ë„ˆëŠ” ìœ ëŠ¥í•œ ì—¬í–‰ ê°€ì´ë“œ",
    prompt="""
ë‹¤ìŒì€ ë§›ì§‘ {{ ë§›ì§‘ }}ì˜ ëŒ€í‘œ ë©”ë‰´.

{% for ë©”ë‰´ in ë©”ë‰´_ë¦¬ìŠ¤íŠ¸ %}
- {{ ë©”ë‰´.ì´ë¦„ }}
{% endfor %}

ë§›ì§‘ ì†Œê°œ ë¸”ë¡œê·¸ ê¸€ì„ ì¨ì¤˜.
    """,
)

reply_generator = llm.ask({
    "ë§›ì§‘": "ì„±ì‹¬ë‹¹",
    "ë©”ë‰´_ë¦¬ìŠ¤íŠ¸": [
        {"ì´ë¦„": "íŠ€ê¹€ì†Œë³´ë¡œ" },
        {"ì´ë¦„": "íŠ€ì†Œêµ¬ë§ˆ" },
        {"ì´ë¦„": "íŒíƒ€ë¡± ë¶€ì¶”ë¹µ" },
        {"ì´ë¦„": "ì‹œë£¨ ì‹œë¦¬ì¦ˆ" },
        {"ì´ë¦„": "ëª…ë€ë°”ê²ŒíŠ¸" },
        {"ì´ë¦„": "ë³´ë¬¸ì‚°ë©”ì•„ë¦¬" },
    ],
}, stream=True)

for reply in reply_generator:
    print(reply.text, end="", flush=True)
print()
print(reply.usage)
```

### íŒŒì¼ ì—…ë¡œë“œ ì§€ì›

> Vision Language Model API ì§€ì • í•„ìš”
 
``` python
from pyhub.llm import OpenAILLM

llm = OpenAILLM()  # ë””í´íŠ¸ : gpt-4o-mini

from pathlib import Path
from typing import Union
from django.core.files import File

# pyhub.llm describe ëª…ë ¹ê³¼ ë™ì¼
files: list[Union[str, Path, File]] = [
    # "./sample1.jpg",  # íŒŒì¼ê²½ë¡œ/URL ë¬¸ìì—´ ë° Path ê°ì²´, ì¥ê³  File ê°ì²´ ì§€ì›
    "https://raw.githubusercontent.com/pyhub-kr/dump-data/refs/heads/main/sample1.jpg",
]
reply = llm.ask("Explain this image in korean", files=files)
print(reply)
```

### ì„ë² ë”© ì§€ì›

> Anthropicë¥¼ ì œì™¸í•œ OpenAI, Google, Upstage, Ollama ì§€ì›.

``` python
# ì„ë² ë”© ì§€ì› (ë‹¨ì¼ ë¬¸ìì—´)
#  - OpenAILLM ë””í´íŠ¸ ì„ë² ë”© ëª¨ë¸ : text-embeddding-3-small
embeded = llm.embed("ë¬¸ì„œ ë‚´ìš© 1")
print(len(embeded.array), "ì°¨ì›")
print(embeded.array[:3], "...")
print(embeded.usage)
```

``` python
# ì„ë² ë”© ì§€ì› (ë‹¨ì¼ ë¬¸ìì—´)
embeded = llm.embed(["ë¬¸ì„œ ë‚´ìš© 2", "ë¬¸ì„œ ë‚´ìš© 3"])
print(len(embeded.arrays), "ê°œ")
print(embeded.arrays[0][:3], "...")
print(embeded.arrays[1][:3], "...")
print(embeded.usage)
```

## ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥

### Postgres ë°ì´í„°ë² ì´ìŠ¤ + pgvector í™•ì¥

supabase ì„œë¹„ìŠ¤ì—ì„œë„ pgvector í™•ì¥ ì§€ì› (ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥)

+ [pgvector ì„¤ì¹˜ ê°€ì´ë“œ](https://ai.pyhub.kr/setup/vector-stores/pgvector/)

``` shell title="ë„ì»¤ë¥¼ í™œìš©í•œ pgvector ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±"
docker run \
    -e POSTGRES_USER=djangouser \
    -e POSTGRES_PASSWORD=djangopw \
    -e POSTGRES_DB=djangodb \
    -p 5432:5432 \
    -d \
    pgvector/pgvector:pg17
```

``` title=".env í¬ë§·"
DATABASE_URL=postgresql://djangouser:djangopw@localhost:5432/djangodb
```

``` title="requirements.txt"
# í•„ìˆ˜
django
pgvector
psycopg2-binary
django-pyhub-rag[all]

# ì˜µì…˜
django-environ
django-extensions
```

### ì¥ê³  í”„ë¡œì íŠ¸ ìƒ˜í”Œ settings

ë¨¼ì € ì¥ê³  í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ì‹  í›„ì— ì•„ë˜ settings ì ìš©

``` python title="mysite/settings.py"
# mysite/settings.py

from pathlib import Path
from environ import Env

BASE_DIR = Path(__file__).resolve().parent.parent

env = Env()

env_path = BASE_DIR / ".env"
if env_path.is_file():
    env.read_env(env_path, overwrite=True)

# ...

INSTALLED_APPS = [
    # ...
    "pyhub.rag",  # django-pyhub-rag ì•±
    "django_extensions",
]
```

ì´ì–´ì„œ `myrag` ì¥ê³  ì•±ì„ ìƒì„±í•˜ì‹  í›„ì—, `INSTALLED_APPS` ë¦¬ìŠ¤íŠ¸ì— `myrag` ì¶”ê°€

``` python title="mysite/settings.py"
INSTALLED_APPS = [
    # ...
    "myrag",      # myrag ì•± ìƒì„± í›„ì— ì¶”ê°€
]
```

`DATABASE_URL` í™˜ê²½ë³€ìˆ˜ ê°’ì„ íŒŒì‹±í•˜ì—¬, ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •ì— ë°˜ì˜ë˜ë„ë¡ `env.db()` ì ìš©

``` python title="mysite/settings.py"
DATABASES = {
    "default": env.db(),  # DATABASE_URL í™˜ê²½ë³€ìˆ˜ íŒŒì‹±
}
```

### pgvector íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ django ëª¨ë¸ ì§€ì›

> `VectorField`ë§Œ ì¶”ê°€í•˜ë©´ ê¸°ì¡´ í”„ë¡œì íŠ¸/ì„œë¹„ìŠ¤ì—ì„œ OK. => ìš´ì˜ ë‹¨ìˆœí™”

### django-pyhub-rag í™œìš©

ê´€ê³„í˜• ë°ì´í„°ì™€ ë² ê²‰ ë°ì´í„°ë¥¼ ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬.

- ì¹´í…Œê³ ë¦¬ í•„í„°ë§ê³¼ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ë‹¨ì¼ ì¿¼ë¦¬ì…‹ìœ¼ë¡œ ì²˜ë¦¬
- ìì—°ìŠ¤ëŸ¬ìš´ JOIN ì—°ì‚° (`select_related`, `prefetch_related` ok)


> ë²¡í„° ëª¨ë¸ ì§€ì› + ì„ë² ë”© ì§€ì› + LLM ì§€ì›

``` python title="myrag/models.py"
from django.db import models
from pyhub.rag.models.postgres import PGVectorDocument

# ë””í´íŠ¸ë¡œ page_content, metadata, embedding í•„ë“œ ì •ì˜
#  - embeding í•„ë“œ ì¬ì •ì˜ : ì„ë² ë”© ì°¨ì› ìˆ˜, ì„ë² ë”© ëª¨ë¸ ë³€ê²½ì´ í•„ìš”í•  ë•Œ
class VectorDocument(PGVectorDocument):
    # category = models.ForeignKey("Category",
    #     on_delete=models.CASCADE)

    class Meta:
        indexes = [
            PGVectorDocument.make_hnsw_index(
                "myrag_vd_emb_idx"
            ),
        ]
```

ì´í›„ ìƒì„±ë˜ëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ì˜ operations ë¦¬ìŠ¤íŠ¸ ì²˜ìŒì— `VectorExtension()` ì¶”ê°€

``` python title="myrag/migrations/0001_initial.py"

from pgvector.django import VectorExtension

class Migration(migrations.Migration):
    operations = [
        VectorExtension()  # vector í™•ì¥ í™œì„±í™”
        # ...
    ]
```

``` python title="ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (by ì½”ì‚¬ì¸ ìœ ì‚¬ë„) - ë””í´íŠ¸ ëª¨ë¸ ë§¤ë‹ˆì €ë¥¼ í†µí•œ ì§€ì›"
qs = VectorDocument.objects.all()  # í•„ìš”í•œ ë§Œí¼ ì¡°ê±´ì„ ì¶”ê°€í•œ ë’¤ì—
docs = qs.similarity_search("ìœ ì‚¬ë¬¸ì„œ ì°¾ì„ ë‚´ìš©")
# docs = await qs.similarity_search_async("ìœ ì‚¬ë¬¸ì„œ ì°¾ì„ ë‚´ìš©")
```

### ì„ë² ë”© + ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥

> page_content í•„ë“œê°€ ë³€ê²½ë˜ë©´, ìë™ìœ¼ë¡œ embedding í•„ë“œ ì—…ë°ì´íŠ¸ ìˆ˜í–‰

ë°©ë²• 1) pyhub.rag ì¥ê³  ê´€ë¦¬ ëª…ë ¹ì„ í†µí•œ ì„ë² ë”© + ë²¡í„° ìŠ¤í† ì–´ ì €ì¥

```
â¯ python manage.py load_jsonl myrag.VectorDocument ./hyundai.jsonl
Created final batch of 84 instances
Successfully created 84 instances in total
```

ë°©ë²• 2) ì¿¼ë¦¬ì…‹ì„ í†µí•œ ì§ì ‘ ì„ë² ë”© + ë²¡í„° ìŠ¤í† ì–´ ì €ì¥

``` python
import json
from myrag.models import VectorDocument

jsonl_path = "./hyundai.jsonl"

vector_document_list = []

for line in open(jsonl_path, "rt", encoding="utf-8"):
    line = line.strip()
    if line:
        obj = json.loads(line)
        # embedding í•„ë“œ ê°’ì´ ì—†ëŠ” ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ì„œëŠ”
        # ëª¨ì•„ì„œ bulk embedding API ìš”ì²­
        vector_document = VectorDocument(
            page_content=obj["page_content"],
            metadata=obj["metadata"],
        )
        vector_document_list.append(vector_document)

VectorDocument.objects.bulk_create(
    vector_document_list,
    batch_size=1000,
)
```

ì €ì¥ëœ ë ˆì½”ë“œ ê°œìˆ˜ í™•ì¸ ë° ì„ë² ë”© í•„ë“œ í™•ì¸

``` python
print(VectorDocument.objects.first().embedding)
print(VectorDocument.objects.all().count())
```

## ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ë° RAG ì±„íŒ…

### ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

embedding í•„ë“œì— ì§€ì •ëœ LLM ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±í•œ í›„ì—, ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

``` python title="[django-extensions] shell_plus --print-sql ëª…ë ¹ìœ¼ë¡œ ì‰˜ êµ¬ë™"
>>> docs = VectorDocument.objects.similarity_search("ë“±ë°›ì´ ê°ë„")
>>> for doc in docs:
...     print(doc.distance, doc.page_content[:100] + '...')
...

SELECT "myrag_vectordocument"."id",
       "myrag_vectordocument"."page_content",
       "myrag_vectordocument"."metadata",
       ("myrag_vectordocument"."embedding" <=> '[0.02761758863925934,
           -0.014913497492671013,
           0.03034409135580063,

Execution time: 0.004041s [Database: default]
```

ì¶œë ¥

```
0.5975217401242773 3

# ë“±ë°›ì´ ê°ë„ ì¡°ì ˆ(2)(ì „ë™ì‹)

ë“±ë°›ì´ ê°ë„ ì¡°ì ˆ ìŠ¤ìœ„ì¹˜(2) ìœ—ë¶€ë¶„ì„ ì•ìœ¼ë¡œ ë‹¹ê¸°ë©´ ë“±ë°›ì´ê°€ ì•ìœ¼ë¡œ ìˆ™ì—¬ì§€ê³ , ë’¤ë¡œ ë‹¹ê¸°ë©´ ë“±
ë°›ì´ê°€ ë’¤ë¡œ ì –í˜€ì§‘ë‹ˆë‹¤.

# ë“±ë°›ì´ ì ‘...
0.6264605820178986 3

- 2: ë“±ë°›ì´ ê°ë„ ì¡°ì ˆ
- 3: ì¢Œì„ ë†’ë‚®ì´ ì¡°ì ˆ(ì‚¬ì–‘ ì ìš© ì‹œ)
- 4: ì¿ ì…˜ ê°ë„ ì¡°ì ˆ(ì‚¬ì–‘ ì ìš© ì‹œ)
- 5: ë‹¤ë¦¬ ë°›ì¹¨ëŒ€ ì¡°ì ˆ ìŠ¤ìœ„ì¹˜(ì‚¬ì–‘ ì ìš© ì‹œ)
- 6: ë¦´ë ‰...
0.6310898817884755 # ì‹œíŠ¸ ë° ì•ˆì „ ì¥ì¹˜

ë“±ë°›ì´ ê°ë„ ì¡°ì ˆ ìŠ¤ìœ„ì¹˜ ìœ—ë¶€ë¶„ì„ ì•ìœ¼ë¡œ ë‹¹ê¸°ë©´ ë“±ë°›ì´ê°€ ì•ìœ¼ë¡œ ìˆ™ì—¬ì§€ê³ , ë’¤ë¡œ ë°€ë©´ ë“±ë°›ì´ê°€
ë’¤ë¡œ ì –í˜€ì§‘ë‹ˆë‹¤.

# ì¿ ì…˜ ê°ë„/ë†’ë‚®ì´ ì¡°ì ˆí•˜ê¸°

![im...
0.6466300767627413 3

ì „ë™ì‹

![image](p021/02-figure.jpg)
2
4

- 1 ë“±ë°›ì´ ê°ë„ ì¡°ì ˆ ë ˆë²„ ë˜ëŠ” ìŠ¤ìœ„ì¹˜(1)ë¡œ ì¢Œì„ ë“±ë°›ì´ë¥¼ ë’¤ë¡œ ì –íˆì‹­ì‹œì˜¤(2).
- 2 í—¤ë“œë ˆìŠ¤íŠ¸...
```

### RAG ì±„íŒ… ë¹„êµ

=== "django-pyhub-rag ë²„ì „"

    ``` python
    # ë³„ë„ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ êµ¬ë™ ì‹œì— ì¥ê³  í”„ë¡œì íŠ¸ ì´ˆê¸° ë¡œë”©
    import os
    from typing import Generator
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mysite.settings')
    import django; django.setup()
    
    # ì¥ê³  í”„ë¡œì íŠ¸ ì´ˆê¸° ë¡œë”© í›„ì— ìˆ˜í–‰
    from pyhub.llm import OpenAILLM, AnthropicLLM, UpstageLLM, GoogleLLM
    from myrag.models import VectorDocument
    
    
    class ChatLLM:
        def __init__(self):
            system_prompt = """í˜„ëŒ€ ì‚°íƒ€í˜ ìë™ì°¨ ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                               ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”. ì‚¬ì‹¤ê³¼ ì˜ê²¬ì„ êµ¬ë³„í•´ì„œ ëŒ€ë‹µí•˜ì„¸ìš”."""
    #       self.llm = OpenAILLM(system_prompt=system_prompt, model="gpt-4o",
    #                            temperature=0.7, max_tokens=8192)
            self.llm = UpstageLLM(system_prompt=system_prompt, model="solar-pro2-preview",
                                  temperature=0.7, max_tokens=8192)
    
        def ask(self, user_input: str) -> Generator[str, None, None]:
            for reply in self.llm.ask(user_input, stream=True):
                yield reply.text


    class RAGDecisionLLM:
        def __init__(self):
            system_prompt = """
                ì‚¬ìš©ì ì§ˆë¬¸ì´ 'í˜„ëŒ€ ì‚°íƒ€í˜ ìë™ì°¨ ë§¤ë‰´ì–¼'ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰(RAG)ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.
                - ë§¤ë‰´ì–¼ì— ë‚˜ì˜¬ ë²•í•œ ì§ˆë¬¸ì´ë©´ "RAG"
                - ì¼ë°˜ì ì¸ ìƒì‹ì´ë‚˜ ë§¤ë‰´ì–¼ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì´ë©´ "NO-RAG
            """
            self.llm = GoogleLLM(system_prompt=system_prompt, model="gemini-2.0-flash-lite")
    
        def should_use_rag(self, question: str) -> bool:
            reply = self.llm.ask(
                input=question,
                # choices ì¸ìê°€ ì§€ì •ë˜ë©´ ê°•ì œë¡œ temperature=0.1ë¡œ ë³€ê²½
                # openai llmì—ì„œëŠ” Structured Outputs ë°©ì‹ìœ¼ë¡œ ë™ì‘
                # https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat
                choices=["RAG", "NO-RAG"],
                use_history=False,
            )
            # return reply.choice == "RAG"
            return reply.choice_index == 0
    ```

=== "ë­ì²´ì¸ ë²„ì „"

    ``` python
    # ë³„ë„ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ êµ¬ë™ ì‹œì— ì¥ê³  í”„ë¡œì íŠ¸ ì´ˆê¸° ë¡œë”©
    import os
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mysite.settings')
    import django; django.setup()
    
    # ì¥ê³  í”„ë¡œì íŠ¸ ì´ˆê¸° ë¡œë”© í›„ì— ìˆ˜í–‰
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.chat_history import InMemoryChatMessageHistory
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    
    from myrag.models import VectorDocument
    
    class ChatLLM:
        def __init__(self):
            llm = ChatOpenAI(model="gpt-4o", temperature=0.7, streaming=True, max_tokens=8192)
            prompt = ChatPromptTemplate.from_messages([
                ("system", """í˜„ëŒ€ ì‚°íƒ€í˜ ìë™ì°¨ ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                              ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ì„¸ìš”. ì‚¬ì‹¤ê³¼ ì˜ê²¬ì„ êµ¬ë³„í•´ì„œ ëŒ€ë‹µí•˜ì„¸ìš”."""),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}")
            ])
            self.history = InMemoryChatMessageHistory()
            self.chain = prompt | llm
    
        def ask(self, user_input: str) -> Generator[str, None, None]:
            stream = self.chain.stream({
                "input": user_input,
                "chat_history": self.history.messages,
            })
    
            full_response = ""
            for chunk in stream:
                yield chunk.content
                full_response += chunk.content
    
            # Add to chat history - store original user input, not the RAG context
            self.history.add_user_message(user_input)
            self.history.add_ai_message(full_response)
    

    class RAGDecisionLLM:
        def __init__(self):
            system_prompt = """
                ì‚¬ìš©ì ì§ˆë¬¸ì´ 'í˜„ëŒ€ ì‚°íƒ€í˜ ìë™ì°¨ ë§¤ë‰´ì–¼'ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰(RAG)ì´ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.
                - ë§¤ë‰´ì–¼ì— ë‚˜ì˜¬ ë²•í•œ ì§ˆë¬¸ì´ë©´ "RAG"
                - ì¼ë°˜ì ì¸ ìƒì‹ì´ë‚˜ ë§¤ë‰´ì–¼ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì´ë©´ "NO-RAG
            """
            llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", temperature=0.1)
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "Question: {question}")
            ])
            self.chain = prompt | llm | StrOutputParser()
    
        def should_use_rag(self, question: str) -> bool:
            result = self.chain.invoke({"question": question})
            choice = result.strip()
            return choice == "RAG"
    ```

ì´í›„ ê³µí†µ ì½”ë“œ

``` python
def main():
    print("ğŸ¤– í˜„ëŒ€ ì‚°íƒ€í˜ AI ì–´ì‹œìŠ¤í„´íŠ¸")

    chat_llm = ChatLLM()
    rag_decision_llm = RAGDecisionLLM()

    while True:
        try:
            user_input = input(">>> ").strip()
            if not user_input: continue

            is_rag = rag_decision_llm.should_use_rag(user_input)

            if is_rag:
                print("INFO: ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ...")
                docs = VectorDocument.objects.similarity_search(user_input)

                # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
                sources = []
                for doc in docs[:5]:  # ìƒìœ„ 5ê°œ ë¬¸ì„œì˜ ì¶œì²˜ë§Œ í‘œì‹œ
                    if hasattr(doc, 'metadata') and doc.metadata:
                        source = doc.metadata.get('source', '?')
                        page = doc.metadata.get('page', '?')
                        sources.append(f"{source}/p.{page}")
                    else:
                        sources.append("?/p.?")

                human_content = f"## Context\n{docs}\n\n##Question#{user_input}"
            else:
                human_content = user_input
                sources = []

            for reply in chat_llm.ask(human_content):
                print(reply, end="", flush=True)

            # RAG ì‚¬ìš© ì‹œ ì¶œì²˜ í‘œì‹œ
            if is_rag and sources:
                print("\n\nğŸ“„ ì¶œì²˜:")
                for source in sources:
                    print(f"  - {source}")
            print()

        except (KeyboardInterrupt, EOFError):
            print('\n\nğŸ‘‹ Goodbye!')
            break


if __name__ == '__main__':
    main()
```

## ìš”ì•½

=== "ì„œë¹„ìŠ¤ ì¤€ë¹„ ë‹¨ê³„"

    ``` sh
    # 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
    pip install -U 'django-pyhub-rag[all]'
    
    # 2) API Key ì„¤ì • (.env)
    UPSTAGE_API_KEY=...
    OPENAI_API_KEY=...
    
    # 3) Document Parse APIë¥¼ í†µí•´ jsonl íŒŒì¼ ìƒì„±
    python -m pyhub.parser upstage -i ./doc.pdf  # pptx ë“±
    
    # 4) VectorDocument ëª¨ë¸ ìƒì„± ë° ë§ˆì´ê·¸ë ˆì´ì…˜
    
    # 5) ì§€ì • VectorDocumentë¡œ ë°ì´í„° ì €ì¥ ë° ì„ë² ë”©
    python manage.py load_jsonl myrag.VectorDocument ./output/doc.jsonl
    ```

=== "ì„œë¹„ìŠ¤ ë‹¨ê³„"
    
    ``` python
    from myrag.models import VectorDocument
    
    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    user_input = "..."
    docs = VectorDocument.objects.similarity_search(user_input)
    human_content = f"## Context\n{docs}\n\n##Question#{user_input}"
    
    from pyhub.llm import OpenAILLM
    
    llm = OpenAILLM(model="gpt-4o")
    reply = llm.ask(human_content)
    print(reply.text)
    print(reply.usage)
    ```
