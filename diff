diff --git a/src/pyhub/llm/commands/__init__.py b/src/pyhub/llm/commands/__init__.py
index ae98af2..188a360 100644
--- a/src/pyhub/llm/commands/__init__.py
+++ b/src/pyhub/llm/commands/__init__.py
@@ -6,6 +6,8 @@ from pyhub import print_for_main
 from .ask import ask
 from .describe import describe
 from .embed import app as embed_app
+from .chat import chat
+from .compare import compare
 
 app = typer.Typer()
 console = Console()
@@ -14,6 +16,8 @@ app.add_typer(embed_app)
 
 app.command()(ask)
 app.command()(describe)
+app.command()(chat)
+app.command()(compare)
 
 
 logo = """
diff --git a/src/pyhub/llm/commands/ask.py b/src/pyhub/llm/commands/ask.py
index 0650f20..099eef4 100644
--- a/src/pyhub/llm/commands/ask.py
+++ b/src/pyhub/llm/commands/ask.py
@@ -9,7 +9,6 @@ from rich.prompt import Prompt
 from rich.table import Table
 
 from pyhub import init
-from pyhub.config import DEFAULT_TOML_PATH, DEFAULT_ENV_PATH
 from pyhub.llm import LLM
 from pyhub.llm.types import LLMChatModelEnum
 
@@ -17,6 +16,7 @@ console = Console()
 
 
 def ask(
+    ctx: typer.Context,
     query: Optional[str] = typer.Argument(None, help="ì§ˆì˜ ë‚´ìš©"),
     model: LLMChatModelEnum = typer.Option(
         LLMChatModelEnum.GPT_4O_MINI,
@@ -37,22 +37,69 @@ def ask(
         "--multi",
         help="ë©€í‹° í„´ ëŒ€í™”",
     ),
-    toml_path: Optional[Path] = typer.Option(
-        DEFAULT_TOML_PATH,
-        "--toml-file",
-        help="toml ì„¤ì • íŒŒì¼ ê²½ë¡œ",
+    output_json: bool = typer.Option(
+        False,
+        "--json",
+        help="JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥ (êµ¬ì¡°í™”ëœ ì‘ë‹µ)",
+    ),
+    schema_path: Optional[Path] = typer.Option(
+        None,
+        "--schema",
+        help="JSON Schema íŒŒì¼ ê²½ë¡œ (êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹ ì •ì˜). "
+        "OpenAIì™€ UpstageëŠ” ì™„ì „í•œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ë©°, "
+        "Anthropic, Google, OllamaëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ì œí•œì  ì§€ì›ë§Œ ì œê³µí•©ë‹ˆë‹¤.",
+    ),
+    output_path: Optional[Path] = typer.Option(
+        None,
+        "--output",
+        "-o",
+        help="ì‘ë‹µì„ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ",
+    ),
+    template_name: Optional[str] = typer.Option(
+        None,
+        "--template",
+        "-t",
+        help="í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ë¦„ (toml íŒŒì¼ì—ì„œ ë¡œë“œ)",
     ),
-    env_path: Optional[Path] = typer.Option(
-        DEFAULT_ENV_PATH,
-        "--env-file",
-        help="í™˜ê²½ ë³€ìˆ˜ íŒŒì¼(.env) ê²½ë¡œ",
+    show_cost: bool = typer.Option(
+        False,
+        "--cost",
+        help="ì˜ˆìƒ ë¹„ìš© í‘œì‹œ",
+    ),
+    show_stats: bool = typer.Option(
+        False,
+        "--stats",
+        help="í† í° ì‚¬ìš©ëŸ‰ ë° ì‘ë‹µ ì‹œê°„ í†µê³„ í‘œì‹œ",
     ),
     is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
 ):
-    """LLMì— ì§ˆì˜í•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
+    """LLMì— ì§ˆì˜í•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤.
+
+    Examples:
+        # ê¸°ë³¸ ì‚¬ìš©
+        pyhub.llm ask "What is Python?"
+
+        # íŒŒì¼ë¡œ ì €ì¥
+        pyhub.llm ask "Explain AI" --output response.txt
+
+        # í…œí”Œë¦¿ ì‚¬ìš©
+        pyhub.llm ask "Fix this code" --template code_review
+
+        # JSON í˜•ì‹ ì¶œë ¥
+        pyhub.llm ask "List 3 colors" --json
+    """
 
     if query is None:
-        query = typer.prompt(">>>", prompt_suffix=" ")
+        if sys.stdin.isatty():
+            # stdinì´ í„°ë¯¸ë„ì¸ ê²½ìš° (íŒŒì´í”„ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš°) - help ì¶œë ¥
+            console.print(ctx.get_help())
+            raise typer.Exit()
+        else:
+            # stdinì—ì„œ ì…ë ¥ì„ ë°›ëŠ” ê²½ìš°
+            console.print("[red]ì˜¤ë¥˜: ì§ˆë¬¸ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
+            console.print('[dim]ì‚¬ìš©ë²•: pyhub.llm ask "ì§ˆë¬¸"[/dim]')
+            console.print('[dim]ë˜ëŠ”: echo "ì»¨í…ìŠ¤íŠ¸" | pyhub.llm ask "ì§ˆë¬¸"[/dim]')
+            raise typer.Exit(1)
 
     # Use stdin as context if available and no context argument was provided
     if context is None and not sys.stdin.isatty():
@@ -76,7 +123,39 @@ def ask(
         log_level = logging.DEBUG
     else:
         log_level = logging.INFO
-    init(debug=True, log_level=log_level, toml_path=toml_path, env_path=env_path)
+    init(debug=True, log_level=log_level)
+
+    # í…œí”Œë¦¿ ë¡œë“œ
+    if template_name:
+        try:
+            import toml
+            from pyhub.config import Config
+
+            toml_path = Config.get_default_toml_path()
+            if toml_path.exists():
+                with toml_path.open("r", encoding="utf-8") as f:
+                    config = toml.load(f)
+                    templates = config.get("prompt_templates", {}).get(template_name, {})
+                    if templates:
+                        system_prompt = templates.get("system", system_prompt)
+                        if "user" in templates and "{query}" in templates["user"]:
+                            query = templates["user"].format(query=query)
+                    else:
+                        console.print(f"[yellow]ê²½ê³ : í…œí”Œë¦¿ '{template_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
+        except Exception as e:
+            console.print(f"[yellow]í…œí”Œë¦¿ ë¡œë“œ ì˜¤ë¥˜: {e}[/yellow]")
+
+    # JSON Schema ë¡œë“œ
+    choices = None
+    if schema_path and schema_path.exists():
+        try:
+            import json
+
+            with schema_path.open("r", encoding="utf-8") as f:
+                choices = json.load(f)
+        except Exception as e:
+            console.print(f"[red]JSON Schema íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}[/red]")
+            raise typer.Exit(1)
 
     if is_verbose:
         table = Table()
@@ -89,8 +168,9 @@ def ask(
         table.add_row("temperature", str(temperature))
         table.add_row("max_tokens", str(max_tokens))
         table.add_row("ë©€í‹° í„´ ì—¬ë¶€", "O" if is_multi else "X")
-        table.add_row("toml_path", f"{toml_path.resolve()} ({"Found" if toml_path.exists() else "Not found"})")
-        table.add_row("env_path", f"{env_path.resolve()} ({"Found" if env_path.exists() else "Not found"})")
+        table.add_row("JSON ì¶œë ¥", "O" if output_json else "X")
+        if schema_path:
+            table.add_row("JSON Schema", str(schema_path))
         console.print(table)
 
     llm = LLM.create(
@@ -102,10 +182,78 @@ def ask(
     if is_verbose:
         console.print(f"Using llm {llm.model}")
 
+    import time
+
+    start_time = time.time()
+
     if not is_multi:
-        for chunk in llm.ask(query, stream=True):
-            console.print(chunk.text, end="")
-        console.print()
+        response_text = ""
+        usage = None
+
+        if output_json or choices:
+            # êµ¬ì¡°í™”ëœ ì‘ë‹µ
+            response = llm.ask(query, choices=choices)
+            usage = response.usage if hasattr(response, "usage") else None
+            if output_json:
+                import json
+
+                response_text = json.dumps(
+                    response.model_dump() if hasattr(response, "model_dump") else response, ensure_ascii=False, indent=2
+                )
+                console.print(response_text)
+            else:
+                response_text = str(response)
+                console.print(response_text)
+        else:
+            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ (í•­ìƒ ìŠ¤íŠ¸ë¦¬ë°)
+            for chunk in llm.ask(query, stream=True):
+                if chunk.text:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶œë ¥
+                    console.print(chunk.text, end="")
+                    response_text += chunk.text
+                if hasattr(chunk, "usage") and chunk.usage:
+                    usage = chunk.usage
+            console.print()
+
+        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
+        elapsed_time = time.time() - start_time
+
+        # íŒŒì¼ë¡œ ì €ì¥
+        if output_path:
+            try:
+                with output_path.open("w", encoding="utf-8") as f:
+                    f.write(response_text)
+                console.print(f"[green]ì‘ë‹µì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
+            except Exception as e:
+                console.print(f"[red]íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}[/red]")
+
+        # í†µê³„ í‘œì‹œ
+        if show_stats and usage:
+            stats_table = Table(title="í†µê³„")
+            stats_table.add_column("í•­ëª©", style="cyan")
+            stats_table.add_column("ê°’", style="green")
+            stats_table.add_row("ì…ë ¥ í† í°", str(usage.input))
+            stats_table.add_row("ì¶œë ¥ í† í°", str(usage.output))
+            stats_table.add_row("ì´ í† í°", str(usage.total))
+            stats_table.add_row("ì‘ë‹µ ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
+            console.print(stats_table)
+
+        # ë¹„ìš© í‘œì‹œ
+        if show_cost and usage:
+            try:
+                from pyhub.llm.utils.pricing import calculate_cost
+
+                cost = calculate_cost(model.value, usage.input, usage.output)
+                cost_table = Table(title="ì˜ˆìƒ ë¹„ìš©")
+                cost_table.add_column("í•­ëª©", style="cyan")
+                cost_table.add_column("ê°’", style="green")
+                cost_table.add_row("ì…ë ¥ ë¹„ìš©", f"${cost['input_cost']:.6f}")
+                cost_table.add_row("ì¶œë ¥ ë¹„ìš©", f"${cost['output_cost']:.6f}")
+                cost_table.add_row("ì´ ë¹„ìš©", f"${cost['total_cost']:.6f}")
+                cost_table.add_row("ì›í™” í™˜ì‚°", f"â‚©{cost['total_cost'] * 1300:.0f}")
+                console.print(cost_table)
+            except Exception as e:
+                if is_verbose:
+                    console.print(f"[yellow]ë¹„ìš© ê³„ì‚° ì‹¤íŒ¨: {e}[/yellow]")
 
     else:
         console.print("Human:", query)
diff --git a/src/pyhub/llm/commands/chat.py b/src/pyhub/llm/commands/chat.py
index de1ba1b..760c8d1 100644
--- a/src/pyhub/llm/commands/chat.py
+++ b/src/pyhub/llm/commands/chat.py
@@ -58,42 +58,48 @@ def chat(
     is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
 ):
     """ëŒ€í™”í˜• LLM ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
-    
+
     if is_verbose:
         log_level = logging.DEBUG
     else:
         log_level = logging.INFO
     init(debug=True, log_level=log_level)
-    
+
     # ì„¸ì…˜ ì •ë³´ í‘œì‹œ
     console.print("\n[bold blue]ğŸ’¬ LLM Chat Session[/bold blue]")
     console.print(f"[dim]ëª¨ë¸: {model.value} | ì˜¨ë„: {temperature} | ìµœëŒ€ í† í°: {max_tokens}[/dim]")
     if system_prompt:
-        console.print(f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt[:50]}...[/dim]" if len(system_prompt) > 50 else f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}[/dim]")
+        console.print(
+            f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt[:50]}...[/dim]"
+            if len(system_prompt) > 50
+            else f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}[/dim]"
+        )
     console.print("[dim]ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', Ctrl+C ë˜ëŠ” Ctrl+Dë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
     console.print("[dim]ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ 'clear'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
     console.print("[dim]í˜„ì¬ ì„¤ì •ì„ ë³´ë ¤ë©´ 'settings'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
     console.print()
-    
+
     # ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥
     base_system_prompt = system_prompt or ""
-    
+
     # ëŒ€í™” íˆìŠ¤í† ë¦¬
     messages = []
     if history_path and history_path.exists():
         try:
             import json
+
             with history_path.open("r", encoding="utf-8") as f:
                 messages = json.load(f)
                 console.print(f"[green]íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(messages)} ë©”ì‹œì§€[/green]\n")
         except Exception as e:
             console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]\n")
-    
+
     # ì´ ì‚¬ìš©ëŸ‰ ì¶”ì 
     from pyhub.llm.types import Usage
+
     total_usage = Usage()
     turn_count = 0
-    
+
     # ëŒ€í™” ë£¨í”„
     while True:
         try:
@@ -104,7 +110,7 @@ def chat(
                 # Ctrl-D ì²˜ë¦¬
                 console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
                 break
-            
+
             # íŠ¹ìˆ˜ ëª…ë ¹ ì²˜ë¦¬
             if user_input.lower() in ["exit", "quit", "bye"]:
                 break
@@ -127,13 +133,13 @@ def chat(
                 settings_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
                 console.print(settings_table)
                 continue
-            
+
             # AI ì‘ë‹µ
             console.print("\n[bold green]AI[/bold green]: ", end="")
-            
+
             response_text = ""
             usage = None
-            
+
             # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
             conversation_context = ""
             if messages:
@@ -141,19 +147,19 @@ def chat(
                 for msg in messages[-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
                     role = "Human" if msg["role"] == "user" else "AI"
                     conversation_context += f"{role}: {msg['content']}\n"
-            
+
             # LLM ì¬ìƒì„± with ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
             current_system_prompt = base_system_prompt
             if conversation_context:
                 current_system_prompt = base_system_prompt + conversation_context
-            
+
             llm = LLM.create(
                 model=model,
                 system_prompt=current_system_prompt,
                 temperature=temperature,
                 max_tokens=max_tokens,
             )
-            
+
             # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
             for chunk in llm.ask(user_input, stream=True):
                 if markdown_mode and "\n" in chunk.text:
@@ -162,30 +168,31 @@ def chat(
                 else:
                     console.print(chunk.text, end="")
                     response_text += chunk.text
-                
-                if hasattr(chunk, 'usage') and chunk.usage:
+
+                if hasattr(chunk, "usage") and chunk.usage:
                     usage = chunk.usage
-            
+
             # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
             if markdown_mode and response_text:
                 console.print()  # ì¤„ë°”ê¿ˆ
                 console.print(Markdown(response_text))
             else:
                 console.print()  # ì¤„ë°”ê¿ˆ
-            
+
             # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
             messages.append({"role": "user", "content": user_input})
             messages.append({"role": "assistant", "content": response_text})
             turn_count += 1
-            
+
             # ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
             if usage:
                 total_usage += usage
-                
+
                 # ë¹„ìš© í‘œì‹œ
                 if show_cost:
                     try:
                         from pyhub.llm.utils.pricing import calculate_cost
+
                         cost = calculate_cost(model.value, usage.input, usage.output)
                         console.print(
                             f"[dim]í† í°: ì…ë ¥ {usage.input}, ì¶œë ¥ {usage.output} | "
@@ -193,18 +200,19 @@ def chat(
                         )
                     except:
                         pass
-            
+
             # íˆìŠ¤í† ë¦¬ ì €ì¥
             if history_path:
                 try:
                     import json
+
                     history_path.parent.mkdir(parents=True, exist_ok=True)
                     with history_path.open("w", encoding="utf-8") as f:
                         json.dump(messages, f, ensure_ascii=False, indent=2)
                 except Exception as e:
                     if is_verbose:
                         console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}[/yellow]")
-        
+
         except KeyboardInterrupt:
             console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
             break
@@ -212,7 +220,7 @@ def chat(
             console.print(f"\n[red]ì˜¤ë¥˜: {e}[/red]")
             if is_verbose:
                 console.print_exception()
-    
+
     # ì„¸ì…˜ ì¢…ë£Œ
     console.print(f"\n[bold blue]ì„¸ì…˜ ì¢…ë£Œ[/bold blue]")
     if turn_count > 0:
@@ -222,16 +230,17 @@ def chat(
         stats_table.add_row("ëŒ€í™” í„´", str(turn_count))
         stats_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
         stats_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
-        
+
         if show_cost:
             try:
                 from pyhub.llm.utils.pricing import calculate_cost
+
                 total_cost = calculate_cost(model.value, total_usage.input, total_usage.output)
                 stats_table.add_row("ì´ ë¹„ìš©", f"${total_cost['total_cost']:.4f}")
                 stats_table.add_row("ì›í™” í™˜ì‚°", f"â‚©{total_cost['total_cost'] * 1300:.0f}")
             except:
                 pass
-        
+
         console.print(stats_table)
-    
+
     console.print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
diff --git a/src/pyhub/llm/commands/compare.py b/src/pyhub/llm/commands/compare.py
index eba00b7..f692916 100644
--- a/src/pyhub/llm/commands/compare.py
+++ b/src/pyhub/llm/commands/compare.py
@@ -66,34 +66,34 @@ def compare(
     is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
 ):
     """ì—¬ëŸ¬ LLM ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•©ë‹ˆë‹¤."""
-    
+
     # queryê°€ ì—†ìœ¼ë©´ help ì¶œë ¥
     if not query:
         console.print(ctx.get_help())
         raise typer.Exit()
-    
+
     if len(models) < 2:
         console.print("[red]ì˜¤ë¥˜: ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.[/red]")
-        console.print("[dim]ì˜ˆ: pyhub.llm compare \"ì§ˆë¬¸\" -m gpt-4o-mini -m claude-3-5-haiku-latest[/dim]")
+        console.print('[dim]ì˜ˆ: pyhub.llm compare "ì§ˆë¬¸" -m gpt-4o-mini -m claude-3-5-haiku-latest[/dim]')
         raise typer.Exit(1)
-    
+
     if is_verbose:
         log_level = logging.DEBUG
     else:
         log_level = logging.INFO
     init(debug=True, log_level=log_level)
-    
+
     # ë¹„êµ ì‹œì‘
     console.print(f"\n[bold blue]ğŸ” ëª¨ë¸ ë¹„êµ[/bold blue]")
     console.print(f"[dim]ì§ˆë¬¸: {query}[/dim]")
     console.print(f"[dim]ëª¨ë¸: {', '.join([m.value for m in models])}[/dim]\n")
-    
+
     results = []
-    
+
     # ê° ëª¨ë¸ë¡œ ì§ˆì˜
     import time
     from concurrent.futures import ThreadPoolExecutor, as_completed
-    
+
     def query_model(model_enum):
         """ë‹¨ì¼ ëª¨ë¸ì— ì§ˆì˜"""
         try:
@@ -103,36 +103,41 @@ def compare(
                 temperature=temperature,
                 max_tokens=max_tokens,
             )
-            
+
             start_time = time.time()
             response_text = ""
             usage = None
-            
+
             # ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ë°›ê¸° (ë¹„êµë¥¼ ìœ„í•´)
             response = llm.ask(query, stream=False)
             response_text = response.text
-            usage = response.usage if hasattr(response, 'usage') else None
-            
+            usage = response.usage if hasattr(response, "usage") else None
+
             elapsed_time = time.time() - start_time
-            
+
             # ë¹„ìš© ê³„ì‚°
             cost_info = None
             if show_cost and usage:
                 try:
                     from pyhub.llm.utils.pricing import calculate_cost
+
                     cost_info = calculate_cost(model_enum.value, usage.input, usage.output)
                 except:
                     pass
-            
+
             return {
                 "model": model_enum.value,
                 "response": response_text,
                 "time": elapsed_time,
-                "usage": {
-                    "input": usage.input if usage else 0,
-                    "output": usage.output if usage else 0,
-                    "total": usage.total if usage else 0,
-                } if usage else None,
+                "usage": (
+                    {
+                        "input": usage.input if usage else 0,
+                        "output": usage.output if usage else 0,
+                        "total": usage.total if usage else 0,
+                    }
+                    if usage
+                    else None
+                ),
                 "cost": cost_info,
                 "success": True,
                 "error": None,
@@ -147,12 +152,12 @@ def compare(
                 "success": False,
                 "error": str(e),
             }
-    
+
     # ë³‘ë ¬ ì²˜ë¦¬
     with console.status("[bold green]ëª¨ë¸ë“¤ì—ê²Œ ì§ˆì˜ ì¤‘...") as status:
         with ThreadPoolExecutor(max_workers=len(models)) as executor:
             future_to_model = {executor.submit(query_model, model): model for model in models}
-            
+
             for future in as_completed(future_to_model):
                 result = future.result()
                 results.append(result)
@@ -160,10 +165,10 @@ def compare(
                     status.update(f"[bold green]âœ“ {result['model']} ì™„ë£Œ[/bold green]")
                 else:
                     status.update(f"[bold red]âœ— {result['model']} ì‹¤íŒ¨[/bold red]")
-    
+
     # ëª¨ë¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
     results = sorted(results, key=lambda x: [m.value for m in models].index(x["model"]))
-    
+
     # ê²°ê³¼ ì¶œë ¥
     if output_format == "side-by-side":
         # ë‚˜ë€íˆ í‘œì‹œ
@@ -177,11 +182,11 @@ def compare(
                     content += f"\n[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]"
             else:
                 content = f"[red]ì˜¤ë¥˜: {result['error']}[/red]"
-            
+
             panels.append(Panel(content, title=result["model"], expand=True))
-        
+
         console.print(Columns(panels, equal=True))
-    
+
     elif output_format == "sequential":
         # ìˆœì°¨ì  í‘œì‹œ
         for result in results:
@@ -194,7 +199,7 @@ def compare(
                     console.print(f"[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]")
             else:
                 console.print(f"[red]ì˜¤ë¥˜: {result['error']}[/red]")
-    
+
     elif output_format == "table":
         # í…Œì´ë¸” í˜•ì‹
         table = Table(title="ëª¨ë¸ ë¹„êµ ê²°ê³¼")
@@ -205,7 +210,7 @@ def compare(
         if show_cost:
             table.add_column("ë¹„ìš©($)", style="magenta")
         table.add_column("í† í°", style="blue")
-        
+
         for result in results:
             if result["success"]:
                 response = result["response"][:100] + "..." if len(result["response"]) > 100 else result["response"]
@@ -222,39 +227,45 @@ def compare(
                 if show_cost:
                     row.append("N/A")
                 row.append("N/A")
-            
+
             table.add_row(*row)
-        
+
         console.print(table)
-    
+
     # í†µê³„ ìš”ì•½
     if show_time or show_cost:
         console.print("\n[bold]í†µê³„ ìš”ì•½[/bold]")
-        
+
         if show_time:
             times = [r["time"] for r in results if r["success"]]
             if times:
-                fastest = min(results, key=lambda x: x["time"] if x["success"] else float('inf'))
+                fastest = min(results, key=lambda x: x["time"] if x["success"] else float("inf"))
                 console.print(f"ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: [green]{fastest['model']}[/green] ({fastest['time']:.2f}ì´ˆ)")
-        
+
         if show_cost:
             costs = [(r["model"], r["cost"]["total_cost"]) for r in results if r["success"] and r["cost"]]
             if costs:
                 cheapest = min(costs, key=lambda x: x[1])
                 console.print(f"ê°€ì¥ ì €ë ´í•œ ëª¨ë¸: [green]{cheapest[0]}[/green] (${cheapest[1]:.6f})")
-    
+
     # ê²°ê³¼ ì €ì¥
     if output_path:
         try:
             import json
+
             with output_path.open("w", encoding="utf-8") as f:
-                json.dump({
-                    "query": query,
-                    "system_prompt": system_prompt,
-                    "temperature": temperature,
-                    "max_tokens": max_tokens,
-                    "results": results,
-                }, f, ensure_ascii=False, indent=2)
+                json.dump(
+                    {
+                        "query": query,
+                        "system_prompt": system_prompt,
+                        "temperature": temperature,
+                        "max_tokens": max_tokens,
+                        "results": results,
+                    },
+                    f,
+                    ensure_ascii=False,
+                    indent=2,
+                )
             console.print(f"\n[green]ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
         except Exception as e:
-            console.print(f"\n[red]ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}[/red]")
\ No newline at end of file
+            console.print(f"\n[red]ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}[/red]")
diff --git a/src/pyhub/llm/commands/describe.py b/src/pyhub/llm/commands/describe.py
index 7f5faba..cf2da7e 100644
--- a/src/pyhub/llm/commands/describe.py
+++ b/src/pyhub/llm/commands/describe.py
@@ -9,7 +9,6 @@ from rich.console import Console
 from rich.table import Table
 
 from pyhub import init
-from pyhub.config import DEFAULT_TOML_PATH, DEFAULT_ENV_PATH
 from pyhub.llm import LLM
 from pyhub.llm.types import LLMChatModelEnum
 
@@ -32,10 +31,10 @@ def validate_image_file(image_path: Path) -> Path:
 
 
 def describe(
-    image_path: Path = typer.Argument(
-        ...,
-        help="ì„¤ëª…ì„ ìš”ì²­í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ",
-        callback=validate_image_file,  # ì½œë°± í•¨ìˆ˜ ì¶”ê°€
+    ctx: typer.Context,
+    image_paths: Optional[list[Path]] = typer.Argument(
+        None,
+        help="ì„¤ëª…ì„ ìš”ì²­í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
     ),
     model: LLMChatModelEnum = typer.Option(
         LLMChatModelEnum.GPT_4O_MINI,
@@ -50,25 +49,56 @@ def describe(
     ),
     temperature: float = typer.Option(0.2, help="LLM ì‘ë‹µì˜ ì˜¨ë„ ì„¤ì • (0.0-2.0, ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì‘ë‹µ)"),
     max_tokens: int = typer.Option(1000, help="ì‘ë‹µì˜ ìµœëŒ€ í† í° ìˆ˜"),
-    toml_path: Optional[Path] = typer.Option(
-        DEFAULT_TOML_PATH,
-        "--toml-file",
-        help="toml ì„¤ì • íŒŒì¼ ê²½ë¡œ",
+    output_format: str = typer.Option(
+        "text",
+        "--format",
+        "-f",
+        help="ì¶œë ¥ í˜•ì‹ (text, json, markdown)",
+    ),
+    output_path: Optional[Path] = typer.Option(
+        None,
+        "--output",
+        "-o",
+        help="ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ",
+    ),
+    batch_output_dir: Optional[Path] = typer.Option(
+        None,
+        "--batch-output-dir",
+        help="ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬",
     ),
-    env_path: Optional[Path] = typer.Option(
-        DEFAULT_ENV_PATH,
-        "--env-file",
-        help="í™˜ê²½ ë³€ìˆ˜ íŒŒì¼(.env) ê²½ë¡œ",
+    show_stats: bool = typer.Option(
+        False,
+        "--stats",
+        help="í† í° ì‚¬ìš©ëŸ‰ í†µê³„ í‘œì‹œ",
     ),
     is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
 ):
     """LLMì—ê²Œ ì´ë¯¸ì§€ ì„¤ëª…ì„ ìš”ì²­í•©ë‹ˆë‹¤."""
 
+    # ì´ë¯¸ì§€ ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° help ì¶œë ¥
+    if not image_paths:
+        console.print(ctx.get_help())
+        raise typer.Exit()
+
+    # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
+    valid_image_paths = []
+    for image_path in image_paths:
+        try:
+            valid_path = validate_image_file(image_path)
+            valid_image_paths.append(valid_path)
+        except typer.BadParameter as e:
+            console.print(f"[red]ì˜¤ë¥˜: {e}[/red]")
+            continue
+
+    if not valid_image_paths:
+        console.print("[red]ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.[/red]")
+        raise typer.Exit(1)
+
     if is_verbose:
         log_level = logging.DEBUG
     else:
         log_level = logging.INFO
-    init(debug=True, log_level=log_level, toml_path=toml_path, env_path=env_path)
+    init(debug=True, log_level=log_level)
 
     # LLM ëª…ë ¹ ì‹œì— PyPDF2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„±ì´ ê±¸ë¦¬ì§€ ì•Šë„ë¡ ì„í¬íŠ¸ ìœ„ì¹˜ ì¡°ì •
     from pyhub.parser.upstage.parser import ImageDescriptor
@@ -76,6 +106,7 @@ def describe(
     if prompt_type is None:
         prompt_templates = ImageDescriptor.get_prompts("describe_image")
     else:
+        toml_path = Path.home() / ".pyhub.toml"
         if not toml_path.exists():
             raise typer.BadParameter(f"{toml_path} íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•´ì£¼ì„¸ìš”. (ëª…ë ¹ ì˜ˆ: pyhub toml --create)")
 
@@ -89,30 +120,141 @@ def describe(
     system_prompt = prompt_templates["system"]
     query = prompt_templates["user"]
 
-    if is_verbose:
-        table = Table()
-        table.add_column("ì„¤ì •", style="cyan")
-        table.add_column("ê°’", style="green")
-        table.add_row("image_path", str(image_path.resolve()))
-        table.add_row("model", model)
-        table.add_row("temperature", str(temperature))
-        table.add_row("max_tokens", str(max_tokens))
-        table.add_row("system prompt", system_prompt)
-        table.add_row("user prompt", query)
-        table.add_row("toml_path", f"{toml_path.resolve()} ({"Found" if toml_path.exists() else "Not found"})")
-        table.add_row("env_path", f"{env_path.resolve()} ({"Found" if env_path.exists() else "Not found"})")
-        console.print(table)
-
-    with image_path.open("rb") as image_file:
-        files = [File(file=image_file)]
-
-        llm = LLM.create(
-            model=model,
-            system_prompt=system_prompt,
-            temperature=temperature,
-            max_tokens=max_tokens,
-        )
-
-        for chunk in llm.ask(query, files=files, stream=True):
-            print(chunk.text, end="", flush=True)
-        print()
+    # ë°°ì¹˜ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
+    if batch_output_dir and len(valid_image_paths) > 1:
+        batch_output_dir.mkdir(parents=True, exist_ok=True)
+
+    # LLM ìƒì„±
+    llm = LLM.create(
+        model=model,
+        system_prompt=system_prompt,
+        temperature=temperature,
+        max_tokens=max_tokens,
+    )
+
+    # ì „ì²´ í†µê³„
+    total_usage = None
+    results = []
+
+    import time
+    from pyhub.llm.types import Usage
+
+    # ê° ì´ë¯¸ì§€ ì²˜ë¦¬
+    for idx, image_path in enumerate(valid_image_paths, 1):
+        if len(valid_image_paths) > 1:
+            console.print(f"\n[bold blue]ì²˜ë¦¬ ì¤‘ ({idx}/{len(valid_image_paths)}): {image_path.name}[/bold blue]")
+
+        start_time = time.time()
+
+        try:
+            with image_path.open("rb") as image_file:
+                files = [File(file=image_file)]
+
+                # ì‘ë‹µ ë°›ê¸°
+                response_text = ""
+                usage = None
+
+                for chunk in llm.ask(query, files=files, stream=True):
+                    response_text += chunk.text
+                    if hasattr(chunk, "usage") and chunk.usage:
+                        usage = chunk.usage
+
+                    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì‹œì—ë§Œ ì‹¤ì‹œê°„ ì¶œë ¥
+                    if len(valid_image_paths) == 1:
+                        console.print(chunk.text, end="")
+
+                elapsed_time = time.time() - start_time
+
+                # ê²°ê³¼ ì €ì¥
+                result = {
+                    "image": str(image_path),
+                    "description": response_text,
+                    "model": model.value,
+                    "elapsed_time": elapsed_time,
+                }
+
+                if usage:
+                    result["usage"] = {
+                        "input_tokens": usage.input,
+                        "output_tokens": usage.output,
+                        "total_tokens": usage.total,
+                    }
+                    # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
+                    if total_usage is None:
+                        total_usage = Usage()
+                    total_usage += usage
+
+                results.append(result)
+
+                # ë°°ì¹˜ ì¶œë ¥ ì‹œ ê°œë³„ íŒŒì¼ ì €ì¥
+                if batch_output_dir and len(valid_image_paths) > 1:
+                    output_file = batch_output_dir / f"{image_path.stem}_description.{output_format}"
+                    save_result(output_file, result, output_format)
+                    if is_verbose:
+                        console.print(f"[dim]ì €ì¥ë¨: {output_file}[/dim]")
+
+                # ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê°„ë‹¨í•œ ê²°ê³¼ í‘œì‹œ
+                if len(valid_image_paths) > 1:
+                    if output_format == "text":
+                        console.print(response_text[:200] + "..." if len(response_text) > 200 else response_text)
+                    else:
+                        console.print(f"[green]âœ“ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)[/green]")
+
+        except Exception as e:
+            console.print(f"[red]ì˜¤ë¥˜ ì²˜ë¦¬ ì¤‘ {image_path}: {e}[/red]")
+            continue
+
+    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì‹œ ì¤„ë°”ê¿ˆ
+    if len(valid_image_paths) == 1:
+        console.print()
+
+    # ì „ì²´ ê²°ê³¼ ì €ì¥
+    if output_path:
+        if len(valid_image_paths) == 1:
+            save_result(output_path, results[0], output_format)
+        else:
+            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ëŠ” í•­ìƒ JSONìœ¼ë¡œ ì €ì¥
+            save_result(output_path, results, "json")
+        console.print(f"[green]ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
+
+    # í†µê³„ í‘œì‹œ
+    if show_stats and total_usage:
+        stats_table = Table(title="ì „ì²´ í†µê³„")
+        stats_table.add_column("í•­ëª©", style="cyan")
+        stats_table.add_column("ê°’", style="green")
+        stats_table.add_row("ì²˜ë¦¬ëœ ì´ë¯¸ì§€", str(len(results)))
+        stats_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
+        stats_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
+        stats_table.add_row("ì´ í† í°", str(total_usage.total))
+        console.print(stats_table)
+
+
+def save_result(output_path: Path, result: dict, format: str):
+    """ê²°ê³¼ë¥¼ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
+    import json
+
+    if format == "json":
+        with output_path.open("w", encoding="utf-8") as f:
+            json.dump(result, f, ensure_ascii=False, indent=2)
+    elif format == "markdown":
+        with output_path.open("w", encoding="utf-8") as f:
+            if isinstance(result, list):
+                # ë°°ì¹˜ ê²°ê³¼
+                for item in result:
+                    f.write(f"## {Path(item['image']).name}\n\n")
+                    f.write(f"{item['description']}\n\n")
+                    f.write("---\n\n")
+            else:
+                # ë‹¨ì¼ ê²°ê³¼
+                f.write(f"# {Path(result['image']).name}\n\n")
+                f.write(f"{result['description']}\n")
+    else:  # text
+        with output_path.open("w", encoding="utf-8") as f:
+            if isinstance(result, list):
+                # ë°°ì¹˜ ê²°ê³¼
+                for item in result:
+                    f.write(f"=== {Path(item['image']).name} ===\n\n")
+                    f.write(f"{item['description']}\n\n")
+            else:
+                # ë‹¨ì¼ ê²°ê³¼
+                f.write(result["description"])
diff --git a/src/pyhub/llm/commands/embed.py b/src/pyhub/llm/commands/embed.py
index 320e18f..3c75adb 100644
--- a/src/pyhub/llm/commands/embed.py
+++ b/src/pyhub/llm/commands/embed.py
@@ -7,15 +7,27 @@ from rich.console import Console
 from rich.table import Table
 
 from pyhub import init
-from pyhub.config import DEFAULT_TOML_PATH, DEFAULT_ENV_PATH
 from pyhub.llm import LLM
 from pyhub.llm.json import json_dumps, json_loads, JSONDecodeError
 from pyhub.llm.types import LLMEmbeddingModelEnum, Usage
 
-app = typer.Typer(name="embed", help="LLM ì„ë² ë”© ê´€ë ¨ ëª…ë ¹")
+app = typer.Typer(
+    name="embed",
+    help="LLM ì„ë² ë”© ê´€ë ¨ ëª…ë ¹",
+    invoke_without_command=True,
+)
 console = Console()
 
 
+@app.callback()
+def embed_callback(ctx: typer.Context):
+    """ì„ë² ë”© ê´€ë ¨ ëª…ë ¹ì–´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
+    if ctx.invoked_subcommand is None:
+        # ì„œë¸Œì»¤ë§¨ë“œê°€ ì—†ìœ¼ë©´ help ì¶œë ¥
+        console.print(ctx.get_help())
+        raise typer.Exit()
+
+
 @app.command()
 def fill_jsonl(
     jsonl_path: Path = typer.Argument(..., help="ì†ŒìŠ¤ JSONL íŒŒì¼ ê²½ë¡œ"),
@@ -31,16 +43,6 @@ def fill_jsonl(
         "-m",
         help="ì„ë² ë”© ëª¨ë¸",
     ),
-    toml_path: Optional[Path] = typer.Option(
-        DEFAULT_TOML_PATH,
-        "--toml-file",
-        help="toml ì„¤ì • íŒŒì¼ ê²½ë¡œ",
-    ),
-    env_path: Optional[Path] = typer.Option(
-        DEFAULT_ENV_PATH,
-        "--env-file",
-        help="í™˜ê²½ ë³€ìˆ˜ íŒŒì¼(.env) ê²½ë¡œ",
-    ),
     is_force: bool = typer.Option(False, "--force", "-f", help="í™•ì¸ ì—†ì´ ì¶œë ¥ í´ë” ì‚­ì œ í›„ ì¬ìƒì„±"),
     is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
 ):
@@ -62,7 +64,7 @@ def fill_jsonl(
         log_level = logging.DEBUG
     else:
         log_level = logging.INFO
-    init(debug=True, log_level=log_level, toml_path=toml_path, env_path=env_path)
+    init(debug=True, log_level=log_level)
 
     llm = LLM.create(embedding_model)
 
@@ -72,8 +74,6 @@ def fill_jsonl(
         table.add_column("ê°’", style="green")
         table.add_row("ì„ë² ë”©ëœ jsonl íŒŒì¼ ìƒì„± ê²½ë¡œ", str(jsonl_out_path))
         table.add_row("ì„ë² ë”© ëª¨ë¸", f"{llm.embedding_model} ({llm.get_embed_size()})")
-        table.add_row("toml íŒŒì¼ ê²½ë¡œ", str(toml_path))
-        table.add_row("í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ", str(env_path))
         console.print(table)
 
     console.print(f"{jsonl_path} ...")
@@ -118,3 +118,340 @@ def fill_jsonl(
     except (IOError, JSONDecodeError) as e:
         console.print(f"[red]íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}[/red]")
         raise typer.Exit(1)
+
+
+@app.command()
+def text(
+    ctx: typer.Context,
+    query: Optional[str] = typer.Argument(None, help="ì„ë² ë”©í•  í…ìŠ¤íŠ¸"),
+    embedding_model: LLMEmbeddingModelEnum = typer.Option(
+        LLMEmbeddingModelEnum.TEXT_EMBEDDING_3_SMALL,
+        "--model",
+        "-m",
+        help="ì„ë² ë”© ëª¨ë¸",
+    ),
+    output_format: str = typer.Option(
+        "json",
+        "--format",
+        "-f",
+        help="ì¶œë ¥ í˜•ì‹ (json, list, numpy)",
+    ),
+    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
+):
+    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
+
+    # queryê°€ ì—†ìœ¼ë©´ help ì¶œë ¥
+    if query is None:
+        console.print(ctx.get_help())
+        raise typer.Exit()
+
+    if is_verbose:
+        log_level = logging.DEBUG
+    else:
+        log_level = logging.INFO
+    init(debug=True, log_level=log_level)
+
+    llm = LLM.create(embedding_model)
+
+    if is_verbose:
+        console.print(f"[dim]ì„ë² ë”© ëª¨ë¸: {llm.embedding_model} (ì°¨ì›: {llm.get_embed_size()})[/dim]")
+        console.print(f"[dim]ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(query)} ë¬¸ì[/dim]")
+
+    # ì„ë² ë”© ìƒì„±
+    embedding_result = llm.embed(query)
+
+    # ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬
+    if output_format == "json":
+        output = {
+            "text": query,
+            "embedding": embedding_result.array,
+            "model": str(llm.embedding_model),
+            "dimensions": len(embedding_result.array),
+            "usage": (
+                {
+                    "input_tokens": embedding_result.usage.input,
+                    "total_tokens": embedding_result.usage.total,
+                }
+                if embedding_result.usage
+                else None
+            ),
+        }
+        console.print(json_dumps(output, indent=2))
+    elif output_format == "list":
+        console.print(embedding_result.array)
+    elif output_format == "numpy":
+        console.print(f"array({embedding_result.array})")
+    else:
+        console.print(f"[red]ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ í˜•ì‹ì…ë‹ˆë‹¤: {output_format}[/red]")
+        raise typer.Exit(1)
+
+
+@app.command()
+def similarity(
+    ctx: typer.Context,
+    text1: Optional[str] = typer.Argument(None, help="ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸"),
+    text2: Optional[str] = typer.Argument(None, help="ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸"),
+    vector1_path: Optional[Path] = typer.Option(
+        None,
+        "--vector1",
+        help="ì²« ë²ˆì§¸ ë²¡í„° íŒŒì¼ (JSON í˜•ì‹)",
+    ),
+    vector2_path: Optional[Path] = typer.Option(
+        None,
+        "--vector2",
+        help="ë‘ ë²ˆì§¸ ë²¡í„° íŒŒì¼ (JSON í˜•ì‹)",
+    ),
+    embedding_model: LLMEmbeddingModelEnum = typer.Option(
+        LLMEmbeddingModelEnum.TEXT_EMBEDDING_3_SMALL,
+        "--model",
+        "-m",
+        help="ì„ë² ë”© ëª¨ë¸ (í…ìŠ¤íŠ¸ ì…ë ¥ ì‹œ)",
+    ),
+    metric: str = typer.Option(
+        "cosine",
+        "--metric",
+        help="ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ (cosine, euclidean, dot)",
+    ),
+    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
+):
+    """ë‘ í…ìŠ¤íŠ¸ ë˜ëŠ” ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
+
+    # ì…ë ¥ ê²€ì¦
+    if not any([text1, text2, vector1_path, vector2_path]):
+        console.print(ctx.get_help())
+        raise typer.Exit()
+
+    if (text1 or text2) and (vector1_path or vector2_path):
+        console.print("[red]ì˜¤ë¥˜: í…ìŠ¤íŠ¸ì™€ ë²¡í„° íŒŒì¼ì„ ë™ì‹œì— ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
+        raise typer.Exit(1)
+
+    if (text1 and not text2) or (text2 and not text1):
+        console.print("[red]ì˜¤ë¥˜: ë‘ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.[/red]")
+        raise typer.Exit(1)
+
+    if (vector1_path and not vector2_path) or (vector2_path and not vector1_path):
+        console.print("[red]ì˜¤ë¥˜: ë‘ ê°œì˜ ë²¡í„° íŒŒì¼ì„ ëª¨ë‘ ì§€ì •í•´ì£¼ì„¸ìš”.[/red]")
+        raise typer.Exit(1)
+
+    if is_verbose:
+        log_level = logging.DEBUG
+    else:
+        log_level = logging.INFO
+    init(debug=True, log_level=log_level)
+
+    # ë²¡í„° ì¤€ë¹„
+    import numpy as np
+
+    if text1 and text2:
+        # í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
+        llm = LLM.create(embedding_model)
+        if is_verbose:
+            console.print(f"[dim]ì„ë² ë”© ëª¨ë¸: {llm.embedding_model}[/dim]")
+            console.print(f"[dim]í…ìŠ¤íŠ¸ 1 ê¸¸ì´: {len(text1)} ë¬¸ì[/dim]")
+            console.print(f"[dim]í…ìŠ¤íŠ¸ 2 ê¸¸ì´: {len(text2)} ë¬¸ì[/dim]")
+
+        embed1 = llm.embed(text1)
+        embed2 = llm.embed(text2)
+        vec1 = np.array(embed1.array)
+        vec2 = np.array(embed2.array)
+    else:
+        # ë²¡í„° íŒŒì¼ ë¡œë“œ
+        try:
+            import json
+
+            with vector1_path.open("r") as f:
+                data1 = json.load(f)
+                vec1 = np.array(data1.get("embedding", data1))
+
+            with vector2_path.open("r") as f:
+                data2 = json.load(f)
+                vec2 = np.array(data2.get("embedding", data2))
+
+        except Exception as e:
+            console.print(f"[red]ë²¡í„° íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}[/red]")
+            raise typer.Exit(1)
+
+    # ìœ ì‚¬ë„ ê³„ì‚°
+    if metric == "cosine":
+        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
+        dot_product = np.dot(vec1, vec2)
+        norm1 = np.linalg.norm(vec1)
+        norm2 = np.linalg.norm(vec2)
+        similarity = dot_product / (norm1 * norm2)
+        distance = 1 - similarity
+    elif metric == "euclidean":
+        # ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬
+        distance = np.linalg.norm(vec1 - vec2)
+        # ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1 ë²”ìœ„)
+        similarity = 1 / (1 + distance)
+    elif metric == "dot":
+        # ë‚´ì 
+        similarity = np.dot(vec1, vec2)
+        distance = -similarity
+    else:
+        console.print(f"[red]ì˜¤ë¥˜: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸¡ì • ë°©ì‹ì…ë‹ˆë‹¤: {metric}[/red]")
+        raise typer.Exit(1)
+
+    # ê²°ê³¼ ì¶œë ¥
+    result_table = Table(title="ìœ ì‚¬ë„ ê³„ì‚° ê²°ê³¼")
+    result_table.add_column("ì¸¡ì • ë°©ì‹", style="cyan")
+    result_table.add_column("ìœ ì‚¬ë„", style="green")
+    result_table.add_column("ê±°ë¦¬", style="yellow")
+    result_table.add_row(metric.capitalize(), f"{similarity:.6f}", f"{distance:.6f}")
+    console.print(result_table)
+
+    # í•´ì„
+    if metric == "cosine":
+        if similarity > 0.9:
+            interpretation = "ë§¤ìš° ìœ ì‚¬í•¨"
+        elif similarity > 0.7:
+            interpretation = "ìœ ì‚¬í•¨"
+        elif similarity > 0.5:
+            interpretation = "ì–´ëŠ ì •ë„ ìœ ì‚¬í•¨"
+        elif similarity > 0.3:
+            interpretation = "ì•½ê°„ ìœ ì‚¬í•¨"
+        else:
+            interpretation = "ìœ ì‚¬í•˜ì§€ ì•ŠìŒ"
+        console.print(f"\ní•´ì„: [bold]{interpretation}[/bold]")
+        console.print("[dim]ì½”ì‚¬ì¸ ìœ ì‚¬ë„: -1 (ì •ë°˜ëŒ€) ~ 0 (ë¬´ê´€) ~ 1 (ë™ì¼)[/dim]")
+
+
+@app.command()
+def batch(
+    ctx: typer.Context,
+    input_file: Optional[Path] = typer.Argument(
+        None,
+        help="ì…ë ¥ íŒŒì¼ ê²½ë¡œ (í…ìŠ¤íŠ¸ íŒŒì¼, í•œ ì¤„ì— í•˜ë‚˜ì”©)",
+    ),
+    output_path: Path = typer.Option(
+        Path("embeddings.jsonl"),
+        "--output",
+        "-o",
+        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (JSONL í˜•ì‹)",
+    ),
+    embedding_model: LLMEmbeddingModelEnum = typer.Option(
+        LLMEmbeddingModelEnum.TEXT_EMBEDDING_3_SMALL,
+        "--model",
+        "-m",
+        help="ì„ë² ë”© ëª¨ë¸",
+    ),
+    batch_size: int = typer.Option(
+        100,
+        "--batch-size",
+        "-b",
+        help="ë°°ì¹˜ í¬ê¸°",
+    ),
+    is_force: bool = typer.Option(False, "--force", "-f", help="ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°"),
+    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸í•œ ì²˜ë¦¬ ì •ë³´ í‘œì‹œ"),
+):
+    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ì¼ê´„ì ìœ¼ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤."""
+
+    # ì…ë ¥ íŒŒì¼ í™•ì¸
+    if not input_file:
+        console.print(ctx.get_help())
+        raise typer.Exit()
+
+    if not input_file.exists():
+        console.print(f"[red]ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}[/red]")
+        raise typer.Exit(1)
+
+    # ì¶œë ¥ íŒŒì¼ í™•ì¸
+    if output_path.exists() and not is_force:
+        console.print(f"[red]ì˜¤ë¥˜: ì¶œë ¥ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}[/red]")
+        console.print("[dim]ë®ì–´ì“°ë ¤ë©´ --force ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.[/dim]")
+        raise typer.Exit(1)
+
+    if is_verbose:
+        log_level = logging.DEBUG
+    else:
+        log_level = logging.INFO
+    init(debug=True, log_level=log_level)
+
+    # LLM ìƒì„±
+    llm = LLM.create(embedding_model)
+
+    if is_verbose:
+        console.print(f"[dim]ì„ë² ë”© ëª¨ë¸: {llm.embedding_model} (ì°¨ì›: {llm.get_embed_size()})[/dim]")
+        console.print(f"[dim]ë°°ì¹˜ í¬ê¸°: {batch_size}[/dim]")
+
+    # í…ìŠ¤íŠ¸ ë¡œë“œ
+    try:
+        with input_file.open("r", encoding="utf-8") as f:
+            texts = [line.strip() for line in f if line.strip()]
+    except Exception as e:
+        console.print(f"[red]ì…ë ¥ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}[/red]")
+        raise typer.Exit(1)
+
+    if not texts:
+        console.print("[yellow]ê²½ê³ : ì…ë ¥ íŒŒì¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
+        raise typer.Exit()
+
+    console.print(f"[blue]ì´ {len(texts)}ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.[/blue]")
+
+    # ë°°ì¹˜ ì²˜ë¦¬
+    total_usage = Usage()
+    results = []
+
+    import time
+
+    start_time = time.time()
+
+    try:
+        with output_path.open("w", encoding="utf-8") as out_f:
+            for i in range(0, len(texts), batch_size):
+                batch = texts[i : i + batch_size]
+                batch_start = time.time()
+
+                # ë°°ì¹˜ ì„ë² ë”© (í˜„ì¬ëŠ” ê°œë³„ ì²˜ë¦¬, ì¶”í›„ ë°°ì¹˜ API ì§€ì› ì‹œ ìˆ˜ì •)
+                batch_results = []
+                batch_usage = Usage()
+
+                for text in batch:
+                    embed_result = llm.embed(text)
+                    batch_results.append(
+                        {
+                            "text": text,
+                            "embedding": embed_result.array,
+                            "model": str(llm.embedding_model),
+                        }
+                    )
+                    if embed_result.usage:
+                        batch_usage += embed_result.usage
+
+                # ê²°ê³¼ ì €ì¥
+                for result in batch_results:
+                    out_f.write(json_dumps(result) + "\n")
+                    results.append(result)
+
+                total_usage += batch_usage
+                batch_time = time.time() - batch_start
+
+                # ì§„í–‰ ìƒí™© í‘œì‹œ
+                progress = min(i + batch_size, len(texts))
+                percentage = (progress / len(texts)) * 100
+                console.print(
+                    f"ì§„í–‰: {percentage:.1f}% ({progress}/{len(texts)}) - "
+                    f"ë°°ì¹˜ ì‹œê°„: {batch_time:.2f}ì´ˆ - "
+                    f"ì´ í† í°: {total_usage.input}",
+                    end="\r",
+                )
+
+        # ì™„ë£Œ
+        total_time = time.time() - start_time
+        console.print()  # ì¤„ë°”ê¿ˆ
+        console.print(f"[green]âœ“ ì„ë² ë”© ì™„ë£Œ![/green]")
+
+        # í†µê³„
+        stats_table = Table(title="ì²˜ë¦¬ í†µê³„")
+        stats_table.add_column("í•­ëª©", style="cyan")
+        stats_table.add_column("ê°’", style="green")
+        stats_table.add_row("ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸", str(len(texts)))
+        stats_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
+        stats_table.add_row("ì²˜ë¦¬ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
+        stats_table.add_row("í‰ê·  ì²˜ë¦¬ ì†ë„", f"{len(texts) / total_time:.1f} í…ìŠ¤íŠ¸/ì´ˆ")
+        stats_table.add_row("ì¶œë ¥ íŒŒì¼", str(output_path))
+        console.print(stats_table)
+
+    except Exception as e:
+        console.print(f"\n[red]ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}[/red]")
+        raise typer.Exit(1)
diff --git a/src/pyhub/llm/openai.py b/src/pyhub/llm/openai.py
index 9fec12e..78c60d2 100644
--- a/src/pyhub/llm/openai.py
+++ b/src/pyhub/llm/openai.py
@@ -244,6 +244,9 @@ class OpenAIMixin:
             request_params,
             cache_alias=self.cache_alias,
         )
+        
+        # Add stream_options after cache key generation
+        request_params["stream_options"] = {"include_usage": True}
 
         if cached_value is not None:
             reply_list = cast(list[Reply], cached_value)
@@ -297,6 +300,9 @@ class OpenAIMixin:
             request_params,
             cache_alias=self.cache_alias,
         )
+        
+        # Add stream_options after cache key generation
+        request_params["stream_options"] = {"include_usage": True}
 
         if cached_value is not None:
             reply_list = cast(list[Reply], cached_value)
diff --git a/src/pyhub/llm/types.py b/src/pyhub/llm/types.py
index 5f61d79..0f3ced8 100644
--- a/src/pyhub/llm/types.py
+++ b/src/pyhub/llm/types.py
@@ -185,6 +185,11 @@ class Usage:
     input: int = 0
     output: int = 0
 
+    @property
+    def total(self) -> int:
+        """ì´ í† í° ìˆ˜ (input + output)"""
+        return self.input + self.output
+
     def __add__(self, other):
         if isinstance(other, Usage):
             return Usage(input=self.input + other.input, output=self.output + other.output)
diff --git a/src/pyhub/llm/utils/mixins.py b/src/pyhub/llm/utils/mixins.py
index 208df8d..9aa3082 100644
--- a/src/pyhub/llm/utils/mixins.py
+++ b/src/pyhub/llm/utils/mixins.py
@@ -15,17 +15,17 @@ console = Console()
 
 class RetryMixin:
     """API í˜¸ì¶œì— ì¬ì‹œë„ ë¡œì§ì„ ì¶”ê°€í•˜ëŠ” ë¯¹ìŠ¤ì¸"""
-    
+
     def __init__(self, *args, enable_retry: bool = True, retry_verbose: bool = False, **kwargs):
         super().__init__(*args, **kwargs)
         self.enable_retry = enable_retry
         self.retry_verbose = retry_verbose
-    
+
     def _wrap_with_retry(self, func: Callable) -> Callable:
         """í•¨ìˆ˜ì— ì¬ì‹œë„ ë¡œì§ì„ ë˜í•‘"""
         if not self.enable_retry:
             return func
-        
+
         @wraps(func)
         def wrapper(*args, **kwargs):
             try:
@@ -44,21 +44,21 @@ class RetryMixin:
                     @retry_api_call(verbose=self.retry_verbose)
                     def retry_func():
                         return func(*args, **kwargs)
-                    
+
                     return retry_func()
-        
+
         return wrapper
-    
+
     def ask(self, *args, **kwargs):
         """ask ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
         original_ask = super().ask
         wrapped_ask = self._wrap_with_retry(original_ask)
         return wrapped_ask(*args, **kwargs)
-    
+
     def ask_async(self, *args, **kwargs):
         """ask_async ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
         original_ask_async = super().ask_async
-        
+
         async def async_wrapper(*args, **kwargs):
             try:
                 return await original_ask_async(*args, **kwargs)
@@ -73,16 +73,17 @@ class RetryMixin:
                 except Exception:
                     # ê°„ë‹¨í•œ ì¬ì‹œë„ (ë¹„ë™ê¸° ë°ì½”ë ˆì´í„°ëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ êµ¬í˜„)
                     import asyncio
+
                     for attempt in range(3):
                         try:
-                            await asyncio.sleep(2 ** attempt)
+                            await asyncio.sleep(2**attempt)
                             return await original_ask_async(*args, **kwargs)
                         except Exception as retry_e:
                             if attempt == 2:
                                 raise retry_e
-        
+
         return async_wrapper(*args, **kwargs)
-    
+
     def embed(self, *args, **kwargs):
         """embed ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
         original_embed = super().embed
@@ -92,10 +93,10 @@ class RetryMixin:
 
 class ValidationMixin:
     """ì…ë ¥ ê²€ì¦ì„ ì¶”ê°€í•˜ëŠ” ë¯¹ìŠ¤ì¸"""
-    
+
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        
+
         # ëª¨ë¸ë³„ í† í° ì œí•œ (ëŒ€ëµì ì¸ ê°’)
         self.model_token_limits = {
             "gpt-4o": 128000,
@@ -105,37 +106,33 @@ class ValidationMixin:
             "gemini-1.5-pro": 1000000,
             "gemini-1.5-flash": 1000000,
         }
-    
+
     def _validate_token_limit(self, text: str, max_tokens: int = None) -> None:
         """í† í° ì œí•œ ê²€ì¦"""
         # ê°„ë‹¨í•œ í† í° ì¶”ì • (ì‹¤ì œë¡œëŠ” tokenizer ì‚¬ìš©í•´ì•¼ í•¨)
         estimated_tokens = len(text) // 4  # ëŒ€ëµ 4ê¸€ìë‹¹ 1í† í°
-        
+
         if max_tokens:
             estimated_total = estimated_tokens + max_tokens
         else:
-            estimated_total = estimated_tokens + getattr(self, 'max_tokens', 1000)
-        
-        model = getattr(self, 'model', '')
+            estimated_total = estimated_tokens + getattr(self, "max_tokens", 1000)
+
+        model = getattr(self, "model", "")
         limit = self.model_token_limits.get(model, 128000)
-        
+
         if estimated_total > limit:
             console.print(
-                f"[yellow]ê²½ê³ : ì˜ˆìƒ í† í° ìˆ˜({estimated_total})ê°€ "
-                f"ëª¨ë¸ ì œí•œ({limit})ì— ê·¼ì ‘í•©ë‹ˆë‹¤.[/yellow]"
+                f"[yellow]ê²½ê³ : ì˜ˆìƒ í† í° ìˆ˜({estimated_total})ê°€ " f"ëª¨ë¸ ì œí•œ({limit})ì— ê·¼ì ‘í•©ë‹ˆë‹¤.[/yellow]"
             )
-    
+
     def ask(self, query: str, *args, **kwargs):
         """ask ë©”ì„œë“œì— ê²€ì¦ ì¶”ê°€"""
-        self._validate_token_limit(query, kwargs.get('max_tokens'))
+        self._validate_token_limit(query, kwargs.get("max_tokens"))
         return super().ask(query, *args, **kwargs)
-    
+
     def embed(self, text: str, *args, **kwargs):
         """embed ë©”ì„œë“œì— ê²€ì¦ ì¶”ê°€"""
         # ì„ë² ë”©ì€ ë³´í†µ 8191 í† í° ì œí•œ
         if len(text) // 4 > 8191:
-            console.print(
-                f"[yellow]ê²½ê³ : í…ìŠ¤íŠ¸ê°€ ì„ë² ë”© í† í° ì œí•œ(8191)ì„ "
-                f"ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.[/yellow]"
-            )
-        return super().embed(text, *args, **kwargs)
\ No newline at end of file
+            console.print(f"[yellow]ê²½ê³ : í…ìŠ¤íŠ¸ê°€ ì„ë² ë”© í† í° ì œí•œ(8191)ì„ " f"ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.[/yellow]")
+        return super().embed(text, *args, **kwargs)
diff --git a/src/pyhub/llm/utils/pricing.py b/src/pyhub/llm/utils/pricing.py
index 154ece7..e4da2cb 100644
--- a/src/pyhub/llm/utils/pricing.py
+++ b/src/pyhub/llm/utils/pricing.py
@@ -9,25 +9,21 @@ PRICING = {
     "chatgpt-4o-latest": {"input": 5.00, "output": 15.00},
     "o1": {"input": 15.00, "output": 60.00},
     "o1-mini": {"input": 3.00, "output": 12.00},
-    
     # Anthropic
     "claude-3-opus-latest": {"input": 15.00, "output": 75.00},
     "claude-3-5-sonnet-latest": {"input": 3.00, "output": 15.00},
     "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
     "claude-3-5-haiku-latest": {"input": 0.80, "output": 4.00},
     "claude-3-5-haiku-20241022": {"input": 0.80, "output": 4.00},
-    
     # Google
     "gemini-2.0-flash": {"input": 0.075, "output": 0.30},
     "gemini-2.0-flash-lite": {"input": 0.015, "output": 0.06},
     "gemini-1.5-flash": {"input": 0.075, "output": 0.30},
     "gemini-1.5-flash-8b": {"input": 0.0375, "output": 0.15},
     "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
-    
     # Upstage
     "solar-pro": {"input": 3.00, "output": 10.00},
     "solar-mini": {"input": 0.30, "output": 0.90},
-    
     # Embedding models
     "text-embedding-3-small": {"input": 0.020, "output": 0},
     "text-embedding-3-large": {"input": 0.130, "output": 0},
@@ -42,12 +38,12 @@ def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> dict:
         pricing = PRICING["gpt-4o-mini"]
     else:
         pricing = PRICING[model]
-    
+
     # 1M í† í°ë‹¹ ê°€ê²©ì´ë¯€ë¡œ ë³€í™˜
     input_cost = (input_tokens / 1_000_000) * pricing["input"]
     output_cost = (output_tokens / 1_000_000) * pricing["output"]
     total_cost = input_cost + output_cost
-    
+
     return {
         "input_cost": input_cost,
         "output_cost": output_cost,
@@ -55,4 +51,4 @@ def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> dict:
         "model": model,
         "input_tokens": input_tokens,
         "output_tokens": output_tokens,
-    }
\ No newline at end of file
+    }
diff --git a/src/pyhub/llm/utils/retry.py b/src/pyhub/llm/utils/retry.py
index b55e362..b80fe42 100644
--- a/src/pyhub/llm/utils/retry.py
+++ b/src/pyhub/llm/utils/retry.py
@@ -12,6 +12,7 @@ console = Console()
 
 class RetryError(Exception):
     """ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
+
     pass
 
 
@@ -26,7 +27,7 @@ def exponential_backoff(
     verbose: bool = False,
 ):
     """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
-    
+
     Args:
         max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
         initial_delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
@@ -37,32 +38,33 @@ def exponential_backoff(
         on_retry: ì¬ì‹œë„ ì‹œ í˜¸ì¶œí•  ì½œë°± í•¨ìˆ˜
         verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
     """
+
     def decorator(func: Callable) -> Callable:
         @wraps(func)
         def wrapper(*args, **kwargs) -> Any:
             last_exception = None
-            
+
             for attempt in range(max_retries + 1):
                 try:
                     return func(*args, **kwargs)
                 except exceptions as e:
                     last_exception = e
-                    
+
                     if attempt == max_retries:
                         # ë§ˆì§€ë§‰ ì‹œë„ì˜€ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
                         raise RetryError(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ {max_retries}íšŒ ì´ˆê³¼") from e
-                    
+
                     # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
-                    delay = min(initial_delay * (exponential_base ** attempt), max_delay)
-                    
+                    delay = min(initial_delay * (exponential_base**attempt), max_delay)
+
                     # ì§€í„° ì¶”ê°€
                     if jitter:
                         delay = delay * (0.5 + random.random())
-                    
+
                     # ì¬ì‹œë„ ì½œë°± í˜¸ì¶œ
                     if on_retry:
                         on_retry(e, attempt + 1)
-                    
+
                     # ë¡œê·¸ ì¶œë ¥
                     if verbose:
                         console.print(
@@ -70,14 +72,15 @@ def exponential_backoff(
                             f"{type(e).__name__}: {str(e)} "
                             f"({delay:.1f}ì´ˆ ëŒ€ê¸°)[/yellow]"
                         )
-                    
+
                     # ëŒ€ê¸°
                     time.sleep(delay)
-            
+
             # ì—¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì•ˆ ë¨
             raise last_exception
-        
+
         return wrapper
+
     return decorator
 
 
@@ -89,7 +92,7 @@ def retry_with_fallback(
     verbose: bool = False,
 ) -> Any:
     """ê¸°ë³¸ í•¨ìˆ˜ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ì¬ì‹œë„ íŒ¨í„´
-    
+
     Args:
         primary_func: ê¸°ë³¸ í•¨ìˆ˜
         fallback_func: ëŒ€ì²´ í•¨ìˆ˜
@@ -99,6 +102,7 @@ def retry_with_fallback(
     """
     # ê¸°ë³¸ í•¨ìˆ˜ ì‹œë„
     try:
+
         @exponential_backoff(
             max_retries=max_retries,
             exceptions=exceptions,
@@ -106,19 +110,20 @@ def retry_with_fallback(
         )
         def try_primary():
             return primary_func()
-        
+
         return try_primary()
-    
+
     except (RetryError, *exceptions) as e:
         if verbose:
             console.print(f"[yellow]ê¸°ë³¸ í•¨ìˆ˜ ì‹¤íŒ¨: {e}[/yellow]")
-        
+
         # ëŒ€ì²´ í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ì‹œë„
         if fallback_func:
             if verbose:
                 console.print("[blue]ëŒ€ì²´ í•¨ìˆ˜ ì‹œë„ ì¤‘...[/blue]")
-            
+
             try:
+
                 @exponential_backoff(
                     max_retries=max_retries,
                     exceptions=exceptions,
@@ -126,9 +131,9 @@ def retry_with_fallback(
                 )
                 def try_fallback():
                     return fallback_func()
-                
+
                 return try_fallback()
-            
+
             except (RetryError, *exceptions) as fallback_e:
                 if verbose:
                     console.print(f"[red]ëŒ€ì²´ í•¨ìˆ˜ë„ ì‹¤íŒ¨: {fallback_e}[/red]")
@@ -145,12 +150,13 @@ def retry_api_call(
     verbose: bool = False,
 ):
     """API í˜¸ì¶œì— íŠ¹í™”ëœ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
-    
+
     ë‹¤ìŒ ì—ëŸ¬ë“¤ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬:
     - Rate limit ì—ëŸ¬: ë” ê¸´ ëŒ€ê¸° ì‹œê°„ê³¼ ë” ë§ì€ ì¬ì‹œë„
     - ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: í‘œì¤€ ì¬ì‹œë„
     - ì¸ì¦ ì—ëŸ¬: ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
     """
+
     def decorator(func: Callable) -> Callable:
         @wraps(func)
         def wrapper(*args, **kwargs) -> Any:
@@ -181,99 +187,104 @@ def retry_api_call(
                     "message": "ì„œë²„ ì˜¤ë¥˜",
                 },
             ]
-            
+
             last_exception = None
-            
+
             for config in error_configs:
                 try:
+
                     @exponential_backoff(
                         max_retries=config["max_retries"],
                         initial_delay=config["initial_delay"],
                         max_delay=config["max_delay"],
                         exceptions=config["exceptions"],
                         verbose=verbose,
-                        on_retry=lambda e, attempt: console.print(
-                            f"[yellow]{config['message']}: {e} "
-                            f"(ì¬ì‹œë„ {attempt}/{config['max_retries']})[/yellow]"
-                        ) if verbose else None,
+                        on_retry=lambda e, attempt: (
+                            console.print(
+                                f"[yellow]{config['message']}: {e} "
+                                f"(ì¬ì‹œë„ {attempt}/{config['max_retries']})[/yellow]"
+                            )
+                            if verbose
+                            else None
+                        ),
                     )
                     def try_call():
                         return func(*args, **kwargs)
-                    
+
                     return try_call()
-                
+
                 except config["exceptions"] as e:
                     last_exception = e
                     continue
                 except Exception as e:
                     # ë‹¤ë¥¸ ì˜ˆì™¸ëŠ” ë°”ë¡œ ì „íŒŒ
                     raise
-            
+
             # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
             if last_exception:
                 raise last_exception
             else:
                 # ì¼ë°˜ í˜¸ì¶œ ì‹œë„
                 return func(*args, **kwargs)
-        
+
         return wrapper
+
     return decorator
 
 
 # ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ë“¤
 class APIError(Exception):
     """API ê´€ë ¨ ê¸°ë³¸ ì˜ˆì™¸"""
+
     pass
 
 
 class RateLimitError(APIError):
     """API ìš”ì²­ ì œí•œ ì´ˆê³¼"""
+
     pass
 
 
 class NetworkError(APIError):
     """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜"""
+
     pass
 
 
 class ServerError(APIError):
     """ì„œë²„ ì˜¤ë¥˜ (5xx)"""
+
     pass
 
 
 class AuthenticationError(APIError):
     """ì¸ì¦ ì˜¤ë¥˜ (ì¬ì‹œë„í•˜ë©´ ì•ˆ ë¨)"""
+
     pass
 
 
 def handle_api_error(e: Exception) -> None:
     """API ì—ëŸ¬ë¥¼ ì ì ˆí•œ ì˜ˆì™¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
     error_message = str(e).lower()
-    
+
     # Rate limit ì—ëŸ¬ íŒ¨í„´
-    if any(pattern in error_message for pattern in [
-        "rate limit", "too many requests", "429", "quota exceeded"
-    ]):
+    if any(pattern in error_message for pattern in ["rate limit", "too many requests", "429", "quota exceeded"]):
         raise RateLimitError(str(e)) from e
-    
+
     # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ íŒ¨í„´
-    elif any(pattern in error_message for pattern in [
-        "connection", "timeout", "network", "dns", "refused"
-    ]):
+    elif any(pattern in error_message for pattern in ["connection", "timeout", "network", "dns", "refused"]):
         raise NetworkError(str(e)) from e
-    
+
     # ì„œë²„ ì—ëŸ¬ íŒ¨í„´
-    elif any(pattern in error_message for pattern in [
-        "500", "502", "503", "504", "server error", "internal error"
-    ]):
+    elif any(pattern in error_message for pattern in ["500", "502", "503", "504", "server error", "internal error"]):
         raise ServerError(str(e)) from e
-    
+
     # ì¸ì¦ ì—ëŸ¬ íŒ¨í„´
-    elif any(pattern in error_message for pattern in [
-        "401", "403", "unauthorized", "forbidden", "api key", "authentication"
-    ]):
+    elif any(
+        pattern in error_message for pattern in ["401", "403", "unauthorized", "forbidden", "api key", "authentication"]
+    ):
         raise AuthenticationError(str(e)) from e
-    
+
     # ê¸°íƒ€ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
     else:
-        raise
\ No newline at end of file
+        raise
diff --git a/src/pyhub/llm/commands/chat.py b/src/pyhub/llm/commands/chat.py
new file mode 100644
index 0000000..de1ba1b
--- /dev/null
+++ b/src/pyhub/llm/commands/chat.py
@@ -0,0 +1,237 @@
+import logging
+from pathlib import Path
+from typing import Optional
+
+import typer
+from rich.console import Console
+from rich.prompt import Prompt
+from rich.table import Table
+from rich.markdown import Markdown
+from rich.syntax import Syntax
+
+from pyhub import init
+from pyhub.llm import LLM
+from pyhub.llm.types import LLMChatModelEnum
+
+console = Console()
+
+
+def chat(
+    model: LLMChatModelEnum = typer.Option(
+        LLMChatModelEnum.GPT_4O_MINI,
+        "--model",
+        "-m",
+        help="LLM Chat ëª¨ë¸",
+    ),
+    system_prompt: str = typer.Option(
+        None,
+        "--system-prompt",
+        "-s",
+        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
+    ),
+    temperature: float = typer.Option(
+        0.7,
+        "--temperature",
+        "-t",
+        help="ì‘ë‹µ ì˜¨ë„ (0.0-2.0)",
+    ),
+    max_tokens: int = typer.Option(
+        2000,
+        "--max-tokens",
+        help="ìµœëŒ€ í† í° ìˆ˜",
+    ),
+    history_path: Optional[Path] = typer.Option(
+        None,
+        "--history",
+        help="ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ ê²½ë¡œ",
+    ),
+    show_cost: bool = typer.Option(
+        False,
+        "--cost",
+        help="ê° ì‘ë‹µì˜ ë¹„ìš© í‘œì‹œ",
+    ),
+    markdown_mode: bool = typer.Option(
+        True,
+        "--markdown/--no-markdown",
+        help="ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ ì‚¬ìš©",
+    ),
+    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
+):
+    """ëŒ€í™”í˜• LLM ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
+    
+    if is_verbose:
+        log_level = logging.DEBUG
+    else:
+        log_level = logging.INFO
+    init(debug=True, log_level=log_level)
+    
+    # ì„¸ì…˜ ì •ë³´ í‘œì‹œ
+    console.print("\n[bold blue]ğŸ’¬ LLM Chat Session[/bold blue]")
+    console.print(f"[dim]ëª¨ë¸: {model.value} | ì˜¨ë„: {temperature} | ìµœëŒ€ í† í°: {max_tokens}[/dim]")
+    if system_prompt:
+        console.print(f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt[:50]}...[/dim]" if len(system_prompt) > 50 else f"[dim]ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}[/dim]")
+    console.print("[dim]ì¢…ë£Œí•˜ë ¤ë©´ 'exit', 'quit', Ctrl+C ë˜ëŠ” Ctrl+Dë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
+    console.print("[dim]ëŒ€í™”ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ 'clear'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
+    console.print("[dim]í˜„ì¬ ì„¤ì •ì„ ë³´ë ¤ë©´ 'settings'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/dim]")
+    console.print()
+    
+    # ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥
+    base_system_prompt = system_prompt or ""
+    
+    # ëŒ€í™” íˆìŠ¤í† ë¦¬
+    messages = []
+    if history_path and history_path.exists():
+        try:
+            import json
+            with history_path.open("r", encoding="utf-8") as f:
+                messages = json.load(f)
+                console.print(f"[green]íˆìŠ¤í† ë¦¬ ë¡œë“œ: {len(messages)} ë©”ì‹œì§€[/green]\n")
+        except Exception as e:
+            console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}[/yellow]\n")
+    
+    # ì´ ì‚¬ìš©ëŸ‰ ì¶”ì 
+    from pyhub.llm.types import Usage
+    total_usage = Usage()
+    turn_count = 0
+    
+    # ëŒ€í™” ë£¨í”„
+    while True:
+        try:
+            # ì‚¬ìš©ì ì…ë ¥
+            try:
+                user_input = Prompt.ask("\n[bold cyan]You[/bold cyan]")
+            except EOFError:
+                # Ctrl-D ì²˜ë¦¬
+                console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
+                break
+            
+            # íŠ¹ìˆ˜ ëª…ë ¹ ì²˜ë¦¬
+            if user_input.lower() in ["exit", "quit", "bye"]:
+                break
+            elif user_input.lower() == "clear":
+                messages = []
+                total_usage = Usage()
+                turn_count = 0
+                console.print("[yellow]ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.[/yellow]")
+                continue
+            elif user_input.lower() == "settings":
+                settings_table = Table(title="í˜„ì¬ ì„¤ì •")
+                settings_table.add_column("í•­ëª©", style="cyan")
+                settings_table.add_column("ê°’", style="green")
+                settings_table.add_row("ëª¨ë¸", model.value)
+                settings_table.add_row("ì˜¨ë„", str(temperature))
+                settings_table.add_row("ìµœëŒ€ í† í°", str(max_tokens))
+                settings_table.add_row("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", system_prompt or "(ì—†ìŒ)")
+                settings_table.add_row("ëŒ€í™” í„´", str(turn_count))
+                settings_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
+                settings_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
+                console.print(settings_table)
+                continue
+            
+            # AI ì‘ë‹µ
+            console.print("\n[bold green]AI[/bold green]: ", end="")
+            
+            response_text = ""
+            usage = None
+            
+            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
+            conversation_context = ""
+            if messages:
+                conversation_context = "\n\nëŒ€í™” íˆìŠ¤í† ë¦¬:\n"
+                for msg in messages[-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
+                    role = "Human" if msg["role"] == "user" else "AI"
+                    conversation_context += f"{role}: {msg['content']}\n"
+            
+            # LLM ì¬ìƒì„± with ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
+            current_system_prompt = base_system_prompt
+            if conversation_context:
+                current_system_prompt = base_system_prompt + conversation_context
+            
+            llm = LLM.create(
+                model=model,
+                system_prompt=current_system_prompt,
+                temperature=temperature,
+                max_tokens=max_tokens,
+            )
+            
+            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
+            for chunk in llm.ask(user_input, stream=True):
+                if markdown_mode and "\n" in chunk.text:
+                    # ë§ˆí¬ë‹¤ìš´ ëª¨ë“œì—ì„œëŠ” ì „ì²´ ì‘ë‹µì„ ëª¨ì•„ì„œ ë Œë”ë§
+                    response_text += chunk.text
+                else:
+                    console.print(chunk.text, end="")
+                    response_text += chunk.text
+                
+                if hasattr(chunk, 'usage') and chunk.usage:
+                    usage = chunk.usage
+            
+            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
+            if markdown_mode and response_text:
+                console.print()  # ì¤„ë°”ê¿ˆ
+                console.print(Markdown(response_text))
+            else:
+                console.print()  # ì¤„ë°”ê¿ˆ
+            
+            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
+            messages.append({"role": "user", "content": user_input})
+            messages.append({"role": "assistant", "content": response_text})
+            turn_count += 1
+            
+            # ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
+            if usage:
+                total_usage += usage
+                
+                # ë¹„ìš© í‘œì‹œ
+                if show_cost:
+                    try:
+                        from pyhub.llm.utils.pricing import calculate_cost
+                        cost = calculate_cost(model.value, usage.input, usage.output)
+                        console.print(
+                            f"[dim]í† í°: ì…ë ¥ {usage.input}, ì¶œë ¥ {usage.output} | "
+                            f"ë¹„ìš©: ${cost['total_cost']:.6f} (â‚©{cost['total_cost'] * 1300:.0f})[/dim]"
+                        )
+                    except:
+                        pass
+            
+            # íˆìŠ¤í† ë¦¬ ì €ì¥
+            if history_path:
+                try:
+                    import json
+                    history_path.parent.mkdir(parents=True, exist_ok=True)
+                    with history_path.open("w", encoding="utf-8") as f:
+                        json.dump(messages, f, ensure_ascii=False, indent=2)
+                except Exception as e:
+                    if is_verbose:
+                        console.print(f"[yellow]íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}[/yellow]")
+        
+        except KeyboardInterrupt:
+            console.print("\n[yellow]ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...[/yellow]")
+            break
+        except Exception as e:
+            console.print(f"\n[red]ì˜¤ë¥˜: {e}[/red]")
+            if is_verbose:
+                console.print_exception()
+    
+    # ì„¸ì…˜ ì¢…ë£Œ
+    console.print(f"\n[bold blue]ì„¸ì…˜ ì¢…ë£Œ[/bold blue]")
+    if turn_count > 0:
+        stats_table = Table(title="ì„¸ì…˜ í†µê³„")
+        stats_table.add_column("í•­ëª©", style="cyan")
+        stats_table.add_column("ê°’", style="green")
+        stats_table.add_row("ëŒ€í™” í„´", str(turn_count))
+        stats_table.add_row("ì´ ì…ë ¥ í† í°", str(total_usage.input))
+        stats_table.add_row("ì´ ì¶œë ¥ í† í°", str(total_usage.output))
+        
+        if show_cost:
+            try:
+                from pyhub.llm.utils.pricing import calculate_cost
+                total_cost = calculate_cost(model.value, total_usage.input, total_usage.output)
+                stats_table.add_row("ì´ ë¹„ìš©", f"${total_cost['total_cost']:.4f}")
+                stats_table.add_row("ì›í™” í™˜ì‚°", f"â‚©{total_cost['total_cost'] * 1300:.0f}")
+            except:
+                pass
+        
+        console.print(stats_table)
+    
+    console.print("\nğŸ‘‹ ì•ˆë…•íˆ ê°€ì„¸ìš”!")
diff --git a/src/pyhub/llm/commands/compare.py b/src/pyhub/llm/commands/compare.py
new file mode 100644
index 0000000..eba00b7
--- /dev/null
+++ b/src/pyhub/llm/commands/compare.py
@@ -0,0 +1,260 @@
+import logging
+from pathlib import Path
+from typing import Optional
+
+import typer
+from rich.console import Console
+from rich.table import Table
+from rich.columns import Columns
+from rich.panel import Panel
+
+from pyhub import init
+from pyhub.llm import LLM
+from pyhub.llm.types import LLMChatModelEnum
+
+console = Console()
+
+
+def compare(
+    ctx: typer.Context,
+    query: Optional[str] = typer.Argument(None, help="ë¹„êµí•  ì§ˆë¬¸"),
+    models: list[LLMChatModelEnum] = typer.Option(
+        [LLMChatModelEnum.GPT_4O_MINI, LLMChatModelEnum.CLAUDE_HAIKU_3_5_LATEST],
+        "--model",
+        "-m",
+        help="ë¹„êµí•  ëª¨ë¸ë“¤ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)",
+    ),
+    system_prompt: str = typer.Option(
+        None,
+        "--system-prompt",
+        "-s",
+        help="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
+    ),
+    temperature: float = typer.Option(
+        0.7,
+        "--temperature",
+        "-t",
+        help="ì‘ë‹µ ì˜¨ë„",
+    ),
+    max_tokens: int = typer.Option(
+        1000,
+        "--max-tokens",
+        help="ìµœëŒ€ í† í° ìˆ˜",
+    ),
+    output_format: str = typer.Option(
+        "side-by-side",
+        "--format",
+        "-f",
+        help="ì¶œë ¥ í˜•ì‹ (side-by-side, sequential, table)",
+    ),
+    output_path: Optional[Path] = typer.Option(
+        None,
+        "--output",
+        "-o",
+        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON)",
+    ),
+    show_cost: bool = typer.Option(
+        True,
+        "--cost/--no-cost",
+        help="ë¹„ìš© ë¹„êµ í‘œì‹œ",
+    ),
+    show_time: bool = typer.Option(
+        True,
+        "--time/--no-time",
+        help="ì‘ë‹µ ì‹œê°„ ë¹„êµ í‘œì‹œ",
+    ),
+    is_verbose: bool = typer.Option(False, "--verbose", help="ìƒì„¸ ì •ë³´ í‘œì‹œ"),
+):
+    """ì—¬ëŸ¬ LLM ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•©ë‹ˆë‹¤."""
+    
+    # queryê°€ ì—†ìœ¼ë©´ help ì¶œë ¥
+    if not query:
+        console.print(ctx.get_help())
+        raise typer.Exit()
+    
+    if len(models) < 2:
+        console.print("[red]ì˜¤ë¥˜: ìµœì†Œ 2ê°œ ì´ìƒì˜ ëª¨ë¸ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.[/red]")
+        console.print("[dim]ì˜ˆ: pyhub.llm compare \"ì§ˆë¬¸\" -m gpt-4o-mini -m claude-3-5-haiku-latest[/dim]")
+        raise typer.Exit(1)
+    
+    if is_verbose:
+        log_level = logging.DEBUG
+    else:
+        log_level = logging.INFO
+    init(debug=True, log_level=log_level)
+    
+    # ë¹„êµ ì‹œì‘
+    console.print(f"\n[bold blue]ğŸ” ëª¨ë¸ ë¹„êµ[/bold blue]")
+    console.print(f"[dim]ì§ˆë¬¸: {query}[/dim]")
+    console.print(f"[dim]ëª¨ë¸: {', '.join([m.value for m in models])}[/dim]\n")
+    
+    results = []
+    
+    # ê° ëª¨ë¸ë¡œ ì§ˆì˜
+    import time
+    from concurrent.futures import ThreadPoolExecutor, as_completed
+    
+    def query_model(model_enum):
+        """ë‹¨ì¼ ëª¨ë¸ì— ì§ˆì˜"""
+        try:
+            llm = LLM.create(
+                model=model_enum,
+                system_prompt=system_prompt,
+                temperature=temperature,
+                max_tokens=max_tokens,
+            )
+            
+            start_time = time.time()
+            response_text = ""
+            usage = None
+            
+            # ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ ë°›ê¸° (ë¹„êµë¥¼ ìœ„í•´)
+            response = llm.ask(query, stream=False)
+            response_text = response.text
+            usage = response.usage if hasattr(response, 'usage') else None
+            
+            elapsed_time = time.time() - start_time
+            
+            # ë¹„ìš© ê³„ì‚°
+            cost_info = None
+            if show_cost and usage:
+                try:
+                    from pyhub.llm.utils.pricing import calculate_cost
+                    cost_info = calculate_cost(model_enum.value, usage.input, usage.output)
+                except:
+                    pass
+            
+            return {
+                "model": model_enum.value,
+                "response": response_text,
+                "time": elapsed_time,
+                "usage": {
+                    "input": usage.input if usage else 0,
+                    "output": usage.output if usage else 0,
+                    "total": usage.total if usage else 0,
+                } if usage else None,
+                "cost": cost_info,
+                "success": True,
+                "error": None,
+            }
+        except Exception as e:
+            return {
+                "model": model_enum.value,
+                "response": None,
+                "time": 0,
+                "usage": None,
+                "cost": None,
+                "success": False,
+                "error": str(e),
+            }
+    
+    # ë³‘ë ¬ ì²˜ë¦¬
+    with console.status("[bold green]ëª¨ë¸ë“¤ì—ê²Œ ì§ˆì˜ ì¤‘...") as status:
+        with ThreadPoolExecutor(max_workers=len(models)) as executor:
+            future_to_model = {executor.submit(query_model, model): model for model in models}
+            
+            for future in as_completed(future_to_model):
+                result = future.result()
+                results.append(result)
+                if result["success"]:
+                    status.update(f"[bold green]âœ“ {result['model']} ì™„ë£Œ[/bold green]")
+                else:
+                    status.update(f"[bold red]âœ— {result['model']} ì‹¤íŒ¨[/bold red]")
+    
+    # ëª¨ë¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬
+    results = sorted(results, key=lambda x: [m.value for m in models].index(x["model"]))
+    
+    # ê²°ê³¼ ì¶œë ¥
+    if output_format == "side-by-side":
+        # ë‚˜ë€íˆ í‘œì‹œ
+        panels = []
+        for result in results:
+            if result["success"]:
+                content = result["response"]
+                if show_time:
+                    content += f"\n\n[dim]ì‘ë‹µ ì‹œê°„: {result['time']:.2f}ì´ˆ[/dim]"
+                if show_cost and result["cost"]:
+                    content += f"\n[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]"
+            else:
+                content = f"[red]ì˜¤ë¥˜: {result['error']}[/red]"
+            
+            panels.append(Panel(content, title=result["model"], expand=True))
+        
+        console.print(Columns(panels, equal=True))
+    
+    elif output_format == "sequential":
+        # ìˆœì°¨ì  í‘œì‹œ
+        for result in results:
+            console.print(f"\n[bold blue]â”â”â” {result['model']} â”â”â”[/bold blue]")
+            if result["success"]:
+                console.print(result["response"])
+                if show_time:
+                    console.print(f"\n[dim]ì‘ë‹µ ì‹œê°„: {result['time']:.2f}ì´ˆ[/dim]")
+                if show_cost and result["cost"]:
+                    console.print(f"[dim]ë¹„ìš©: ${result['cost']['total_cost']:.6f}[/dim]")
+            else:
+                console.print(f"[red]ì˜¤ë¥˜: {result['error']}[/red]")
+    
+    elif output_format == "table":
+        # í…Œì´ë¸” í˜•ì‹
+        table = Table(title="ëª¨ë¸ ë¹„êµ ê²°ê³¼")
+        table.add_column("ëª¨ë¸", style="cyan")
+        table.add_column("ì‘ë‹µ", style="green", overflow="fold")
+        if show_time:
+            table.add_column("ì‹œê°„(ì´ˆ)", style="yellow")
+        if show_cost:
+            table.add_column("ë¹„ìš©($)", style="magenta")
+        table.add_column("í† í°", style="blue")
+        
+        for result in results:
+            if result["success"]:
+                response = result["response"][:100] + "..." if len(result["response"]) > 100 else result["response"]
+                row = [result["model"], response]
+                if show_time:
+                    row.append(f"{result['time']:.2f}")
+                if show_cost:
+                    row.append(f"{result['cost']['total_cost']:.6f}" if result["cost"] else "N/A")
+                row.append(f"{result['usage']['total']}" if result["usage"] else "N/A")
+            else:
+                row = [result["model"], f"[red]ì˜¤ë¥˜: {result['error']}[/red]"]
+                if show_time:
+                    row.append("N/A")
+                if show_cost:
+                    row.append("N/A")
+                row.append("N/A")
+            
+            table.add_row(*row)
+        
+        console.print(table)
+    
+    # í†µê³„ ìš”ì•½
+    if show_time or show_cost:
+        console.print("\n[bold]í†µê³„ ìš”ì•½[/bold]")
+        
+        if show_time:
+            times = [r["time"] for r in results if r["success"]]
+            if times:
+                fastest = min(results, key=lambda x: x["time"] if x["success"] else float('inf'))
+                console.print(f"ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: [green]{fastest['model']}[/green] ({fastest['time']:.2f}ì´ˆ)")
+        
+        if show_cost:
+            costs = [(r["model"], r["cost"]["total_cost"]) for r in results if r["success"] and r["cost"]]
+            if costs:
+                cheapest = min(costs, key=lambda x: x[1])
+                console.print(f"ê°€ì¥ ì €ë ´í•œ ëª¨ë¸: [green]{cheapest[0]}[/green] (${cheapest[1]:.6f})")
+    
+    # ê²°ê³¼ ì €ì¥
+    if output_path:
+        try:
+            import json
+            with output_path.open("w", encoding="utf-8") as f:
+                json.dump({
+                    "query": query,
+                    "system_prompt": system_prompt,
+                    "temperature": temperature,
+                    "max_tokens": max_tokens,
+                    "results": results,
+                }, f, ensure_ascii=False, indent=2)
+            console.print(f"\n[green]ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}[/green]")
+        except Exception as e:
+            console.print(f"\n[red]ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}[/red]")
\ No newline at end of file
diff --git a/src/pyhub/llm/utils/mixins.py b/src/pyhub/llm/utils/mixins.py
new file mode 100644
index 0000000..208df8d
--- /dev/null
+++ b/src/pyhub/llm/utils/mixins.py
@@ -0,0 +1,141 @@
+"""LLM ê´€ë ¨ ë¯¹ìŠ¤ì¸ í´ë˜ìŠ¤ë“¤"""
+
+from typing import Any, Callable
+from functools import wraps
+
+from .retry import (
+    retry_api_call,
+    handle_api_error,
+    AuthenticationError,
+)
+from rich.console import Console
+
+console = Console()
+
+
+class RetryMixin:
+    """API í˜¸ì¶œì— ì¬ì‹œë„ ë¡œì§ì„ ì¶”ê°€í•˜ëŠ” ë¯¹ìŠ¤ì¸"""
+    
+    def __init__(self, *args, enable_retry: bool = True, retry_verbose: bool = False, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.enable_retry = enable_retry
+        self.retry_verbose = retry_verbose
+    
+    def _wrap_with_retry(self, func: Callable) -> Callable:
+        """í•¨ìˆ˜ì— ì¬ì‹œë„ ë¡œì§ì„ ë˜í•‘"""
+        if not self.enable_retry:
+            return func
+        
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            try:
+                return func(*args, **kwargs)
+            except Exception as e:
+                # API ì—ëŸ¬ë¥¼ ì ì ˆí•œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
+                try:
+                    handle_api_error(e)
+                except AuthenticationError:
+                    # ì¸ì¦ ì—ëŸ¬ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
+                    console.print(f"[red]ì¸ì¦ ì˜¤ë¥˜: {e}[/red]")
+                    console.print("[yellow]API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.[/yellow]")
+                    raise
+                except Exception as converted_e:
+                    # ë³€í™˜ëœ ì—ëŸ¬ë¡œ ì¬ì‹œë„
+                    @retry_api_call(verbose=self.retry_verbose)
+                    def retry_func():
+                        return func(*args, **kwargs)
+                    
+                    return retry_func()
+        
+        return wrapper
+    
+    def ask(self, *args, **kwargs):
+        """ask ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
+        original_ask = super().ask
+        wrapped_ask = self._wrap_with_retry(original_ask)
+        return wrapped_ask(*args, **kwargs)
+    
+    def ask_async(self, *args, **kwargs):
+        """ask_async ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
+        original_ask_async = super().ask_async
+        
+        async def async_wrapper(*args, **kwargs):
+            try:
+                return await original_ask_async(*args, **kwargs)
+            except Exception as e:
+                # ë¹„ë™ê¸° ë²„ì „ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
+                try:
+                    handle_api_error(e)
+                except AuthenticationError:
+                    console.print(f"[red]ì¸ì¦ ì˜¤ë¥˜: {e}[/red]")
+                    console.print("[yellow]API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.[/yellow]")
+                    raise
+                except Exception:
+                    # ê°„ë‹¨í•œ ì¬ì‹œë„ (ë¹„ë™ê¸° ë°ì½”ë ˆì´í„°ëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨íˆ êµ¬í˜„)
+                    import asyncio
+                    for attempt in range(3):
+                        try:
+                            await asyncio.sleep(2 ** attempt)
+                            return await original_ask_async(*args, **kwargs)
+                        except Exception as retry_e:
+                            if attempt == 2:
+                                raise retry_e
+        
+        return async_wrapper(*args, **kwargs)
+    
+    def embed(self, *args, **kwargs):
+        """embed ë©”ì„œë“œì— ì¬ì‹œë„ ë¡œì§ ì ìš©"""
+        original_embed = super().embed
+        wrapped_embed = self._wrap_with_retry(original_embed)
+        return wrapped_embed(*args, **kwargs)
+
+
+class ValidationMixin:
+    """ì…ë ¥ ê²€ì¦ì„ ì¶”ê°€í•˜ëŠ” ë¯¹ìŠ¤ì¸"""
+    
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        
+        # ëª¨ë¸ë³„ í† í° ì œí•œ (ëŒ€ëµì ì¸ ê°’)
+        self.model_token_limits = {
+            "gpt-4o": 128000,
+            "gpt-4o-mini": 128000,
+            "claude-3-5-sonnet-latest": 200000,
+            "claude-3-5-haiku-latest": 200000,
+            "gemini-1.5-pro": 1000000,
+            "gemini-1.5-flash": 1000000,
+        }
+    
+    def _validate_token_limit(self, text: str, max_tokens: int = None) -> None:
+        """í† í° ì œí•œ ê²€ì¦"""
+        # ê°„ë‹¨í•œ í† í° ì¶”ì • (ì‹¤ì œë¡œëŠ” tokenizer ì‚¬ìš©í•´ì•¼ í•¨)
+        estimated_tokens = len(text) // 4  # ëŒ€ëµ 4ê¸€ìë‹¹ 1í† í°
+        
+        if max_tokens:
+            estimated_total = estimated_tokens + max_tokens
+        else:
+            estimated_total = estimated_tokens + getattr(self, 'max_tokens', 1000)
+        
+        model = getattr(self, 'model', '')
+        limit = self.model_token_limits.get(model, 128000)
+        
+        if estimated_total > limit:
+            console.print(
+                f"[yellow]ê²½ê³ : ì˜ˆìƒ í† í° ìˆ˜({estimated_total})ê°€ "
+                f"ëª¨ë¸ ì œí•œ({limit})ì— ê·¼ì ‘í•©ë‹ˆë‹¤.[/yellow]"
+            )
+    
+    def ask(self, query: str, *args, **kwargs):
+        """ask ë©”ì„œë“œì— ê²€ì¦ ì¶”ê°€"""
+        self._validate_token_limit(query, kwargs.get('max_tokens'))
+        return super().ask(query, *args, **kwargs)
+    
+    def embed(self, text: str, *args, **kwargs):
+        """embed ë©”ì„œë“œì— ê²€ì¦ ì¶”ê°€"""
+        # ì„ë² ë”©ì€ ë³´í†µ 8191 í† í° ì œí•œ
+        if len(text) // 4 > 8191:
+            console.print(
+                f"[yellow]ê²½ê³ : í…ìŠ¤íŠ¸ê°€ ì„ë² ë”© í† í° ì œí•œ(8191)ì„ "
+                f"ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.[/yellow]"
+            )
+        return super().embed(text, *args, **kwargs)
\ No newline at end of file
diff --git a/src/pyhub/llm/utils/pricing.py b/src/pyhub/llm/utils/pricing.py
new file mode 100644
index 0000000..154ece7
--- /dev/null
+++ b/src/pyhub/llm/utils/pricing.py
@@ -0,0 +1,58 @@
+"""LLM ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ ë° ë¹„ìš© ê³„ì‚° ìœ í‹¸ë¦¬í‹°"""
+
+# ëª¨ë¸ë³„ ê°€ê²© ì •ë³´ (USD per 1M tokens)
+# 2024ë…„ ê¸°ì¤€
+PRICING = {
+    # OpenAI
+    "gpt-4o": {"input": 2.50, "output": 10.00},
+    "gpt-4o-mini": {"input": 0.150, "output": 0.600},
+    "chatgpt-4o-latest": {"input": 5.00, "output": 15.00},
+    "o1": {"input": 15.00, "output": 60.00},
+    "o1-mini": {"input": 3.00, "output": 12.00},
+    
+    # Anthropic
+    "claude-3-opus-latest": {"input": 15.00, "output": 75.00},
+    "claude-3-5-sonnet-latest": {"input": 3.00, "output": 15.00},
+    "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
+    "claude-3-5-haiku-latest": {"input": 0.80, "output": 4.00},
+    "claude-3-5-haiku-20241022": {"input": 0.80, "output": 4.00},
+    
+    # Google
+    "gemini-2.0-flash": {"input": 0.075, "output": 0.30},
+    "gemini-2.0-flash-lite": {"input": 0.015, "output": 0.06},
+    "gemini-1.5-flash": {"input": 0.075, "output": 0.30},
+    "gemini-1.5-flash-8b": {"input": 0.0375, "output": 0.15},
+    "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
+    
+    # Upstage
+    "solar-pro": {"input": 3.00, "output": 10.00},
+    "solar-mini": {"input": 0.30, "output": 0.90},
+    
+    # Embedding models
+    "text-embedding-3-small": {"input": 0.020, "output": 0},
+    "text-embedding-3-large": {"input": 0.130, "output": 0},
+    "text-embedding-ada-002": {"input": 0.100, "output": 0},
+}
+
+
+def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> dict:
+    """ëª¨ë¸ê³¼ í† í° ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ìš© ê³„ì‚°"""
+    if model not in PRICING:
+        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì€ gpt-4o-mini ê°€ê²©ìœ¼ë¡œ ì¶”ì •
+        pricing = PRICING["gpt-4o-mini"]
+    else:
+        pricing = PRICING[model]
+    
+    # 1M í† í°ë‹¹ ê°€ê²©ì´ë¯€ë¡œ ë³€í™˜
+    input_cost = (input_tokens / 1_000_000) * pricing["input"]
+    output_cost = (output_tokens / 1_000_000) * pricing["output"]
+    total_cost = input_cost + output_cost
+    
+    return {
+        "input_cost": input_cost,
+        "output_cost": output_cost,
+        "total_cost": total_cost,
+        "model": model,
+        "input_tokens": input_tokens,
+        "output_tokens": output_tokens,
+    }
\ No newline at end of file
diff --git a/src/pyhub/llm/utils/retry.py b/src/pyhub/llm/utils/retry.py
new file mode 100644
index 0000000..b55e362
--- /dev/null
+++ b/src/pyhub/llm/utils/retry.py
@@ -0,0 +1,279 @@
+"""ì¬ì‹œë„ ë° ì—ëŸ¬ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°"""
+
+import time
+from functools import wraps
+from typing import Any, Callable, Type, Union
+import random
+
+from rich.console import Console
+
+console = Console()
+
+
+class RetryError(Exception):
+    """ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
+    pass
+
+
+def exponential_backoff(
+    max_retries: int = 3,
+    initial_delay: float = 1.0,
+    max_delay: float = 60.0,
+    exponential_base: float = 2.0,
+    jitter: bool = True,
+    exceptions: tuple[Type[Exception], ...] = (Exception,),
+    on_retry: Callable[[Exception, int], None] = None,
+    verbose: bool = False,
+):
+    """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
+    
+    Args:
+        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
+        initial_delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
+        max_delay: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
+        exponential_base: ì§€ìˆ˜ ì¦ê°€ ë² ì´ìŠ¤
+        jitter: ì§€í„° ì¶”ê°€ ì—¬ë¶€ (ëŒ€ê¸° ì‹œê°„ì— ëœë¤ì„± ì¶”ê°€)
+        exceptions: ì¬ì‹œë„í•  ì˜ˆì™¸ íƒ€ì…ë“¤
+        on_retry: ì¬ì‹œë„ ì‹œ í˜¸ì¶œí•  ì½œë°± í•¨ìˆ˜
+        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
+    """
+    def decorator(func: Callable) -> Callable:
+        @wraps(func)
+        def wrapper(*args, **kwargs) -> Any:
+            last_exception = None
+            
+            for attempt in range(max_retries + 1):
+                try:
+                    return func(*args, **kwargs)
+                except exceptions as e:
+                    last_exception = e
+                    
+                    if attempt == max_retries:
+                        # ë§ˆì§€ë§‰ ì‹œë„ì˜€ìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
+                        raise RetryError(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ {max_retries}íšŒ ì´ˆê³¼") from e
+                    
+                    # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
+                    delay = min(initial_delay * (exponential_base ** attempt), max_delay)
+                    
+                    # ì§€í„° ì¶”ê°€
+                    if jitter:
+                        delay = delay * (0.5 + random.random())
+                    
+                    # ì¬ì‹œë„ ì½œë°± í˜¸ì¶œ
+                    if on_retry:
+                        on_retry(e, attempt + 1)
+                    
+                    # ë¡œê·¸ ì¶œë ¥
+                    if verbose:
+                        console.print(
+                            f"[yellow]ì¬ì‹œë„ {attempt + 1}/{max_retries}: "
+                            f"{type(e).__name__}: {str(e)} "
+                            f"({delay:.1f}ì´ˆ ëŒ€ê¸°)[/yellow]"
+                        )
+                    
+                    # ëŒ€ê¸°
+                    time.sleep(delay)
+            
+            # ì—¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì•ˆ ë¨
+            raise last_exception
+        
+        return wrapper
+    return decorator
+
+
+def retry_with_fallback(
+    primary_func: Callable,
+    fallback_func: Callable = None,
+    max_retries: int = 2,
+    exceptions: tuple[Type[Exception], ...] = (Exception,),
+    verbose: bool = False,
+) -> Any:
+    """ê¸°ë³¸ í•¨ìˆ˜ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ í•¨ìˆ˜ë¥¼ ì‹œë„í•˜ëŠ” ì¬ì‹œë„ íŒ¨í„´
+    
+    Args:
+        primary_func: ê¸°ë³¸ í•¨ìˆ˜
+        fallback_func: ëŒ€ì²´ í•¨ìˆ˜
+        max_retries: ê° í•¨ìˆ˜ì˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
+        exceptions: ì¬ì‹œë„í•  ì˜ˆì™¸ íƒ€ì…ë“¤
+        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
+    """
+    # ê¸°ë³¸ í•¨ìˆ˜ ì‹œë„
+    try:
+        @exponential_backoff(
+            max_retries=max_retries,
+            exceptions=exceptions,
+            verbose=verbose,
+        )
+        def try_primary():
+            return primary_func()
+        
+        return try_primary()
+    
+    except (RetryError, *exceptions) as e:
+        if verbose:
+            console.print(f"[yellow]ê¸°ë³¸ í•¨ìˆ˜ ì‹¤íŒ¨: {e}[/yellow]")
+        
+        # ëŒ€ì²´ í•¨ìˆ˜ê°€ ìˆìœ¼ë©´ ì‹œë„
+        if fallback_func:
+            if verbose:
+                console.print("[blue]ëŒ€ì²´ í•¨ìˆ˜ ì‹œë„ ì¤‘...[/blue]")
+            
+            try:
+                @exponential_backoff(
+                    max_retries=max_retries,
+                    exceptions=exceptions,
+                    verbose=verbose,
+                )
+                def try_fallback():
+                    return fallback_func()
+                
+                return try_fallback()
+            
+            except (RetryError, *exceptions) as fallback_e:
+                if verbose:
+                    console.print(f"[red]ëŒ€ì²´ í•¨ìˆ˜ë„ ì‹¤íŒ¨: {fallback_e}[/red]")
+                raise
+        else:
+            raise
+
+
+# API ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹í™”ëœ ë°ì½”ë ˆì´í„°
+def retry_api_call(
+    max_retries: int = 3,
+    rate_limit_retries: int = 5,
+    network_error_retries: int = 3,
+    verbose: bool = False,
+):
+    """API í˜¸ì¶œì— íŠ¹í™”ëœ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
+    
+    ë‹¤ìŒ ì—ëŸ¬ë“¤ì„ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬:
+    - Rate limit ì—ëŸ¬: ë” ê¸´ ëŒ€ê¸° ì‹œê°„ê³¼ ë” ë§ì€ ì¬ì‹œë„
+    - ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: í‘œì¤€ ì¬ì‹œë„
+    - ì¸ì¦ ì—ëŸ¬: ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
+    """
+    def decorator(func: Callable) -> Callable:
+        @wraps(func)
+        def wrapper(*args, **kwargs) -> Any:
+            # ì—ëŸ¬ íƒ€ì…ë³„ ì¬ì‹œë„ ì„¤ì •
+            error_configs = [
+                # Rate limit ì—ëŸ¬
+                {
+                    "exceptions": (RateLimitError,),
+                    "max_retries": rate_limit_retries,
+                    "initial_delay": 5.0,
+                    "max_delay": 300.0,  # 5ë¶„
+                    "message": "API ìš”ì²­ ì œí•œ ë„ë‹¬",
+                },
+                # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬
+                {
+                    "exceptions": (NetworkError, TimeoutError),
+                    "max_retries": network_error_retries,
+                    "initial_delay": 1.0,
+                    "max_delay": 30.0,
+                    "message": "ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜",
+                },
+                # ì„œë²„ ì—ëŸ¬
+                {
+                    "exceptions": (ServerError,),
+                    "max_retries": max_retries,
+                    "initial_delay": 2.0,
+                    "max_delay": 60.0,
+                    "message": "ì„œë²„ ì˜¤ë¥˜",
+                },
+            ]
+            
+            last_exception = None
+            
+            for config in error_configs:
+                try:
+                    @exponential_backoff(
+                        max_retries=config["max_retries"],
+                        initial_delay=config["initial_delay"],
+                        max_delay=config["max_delay"],
+                        exceptions=config["exceptions"],
+                        verbose=verbose,
+                        on_retry=lambda e, attempt: console.print(
+                            f"[yellow]{config['message']}: {e} "
+                            f"(ì¬ì‹œë„ {attempt}/{config['max_retries']})[/yellow]"
+                        ) if verbose else None,
+                    )
+                    def try_call():
+                        return func(*args, **kwargs)
+                    
+                    return try_call()
+                
+                except config["exceptions"] as e:
+                    last_exception = e
+                    continue
+                except Exception as e:
+                    # ë‹¤ë¥¸ ì˜ˆì™¸ëŠ” ë°”ë¡œ ì „íŒŒ
+                    raise
+            
+            # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
+            if last_exception:
+                raise last_exception
+            else:
+                # ì¼ë°˜ í˜¸ì¶œ ì‹œë„
+                return func(*args, **kwargs)
+        
+        return wrapper
+    return decorator
+
+
+# ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ë“¤
+class APIError(Exception):
+    """API ê´€ë ¨ ê¸°ë³¸ ì˜ˆì™¸"""
+    pass
+
+
+class RateLimitError(APIError):
+    """API ìš”ì²­ ì œí•œ ì´ˆê³¼"""
+    pass
+
+
+class NetworkError(APIError):
+    """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜"""
+    pass
+
+
+class ServerError(APIError):
+    """ì„œë²„ ì˜¤ë¥˜ (5xx)"""
+    pass
+
+
+class AuthenticationError(APIError):
+    """ì¸ì¦ ì˜¤ë¥˜ (ì¬ì‹œë„í•˜ë©´ ì•ˆ ë¨)"""
+    pass
+
+
+def handle_api_error(e: Exception) -> None:
+    """API ì—ëŸ¬ë¥¼ ì ì ˆí•œ ì˜ˆì™¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
+    error_message = str(e).lower()
+    
+    # Rate limit ì—ëŸ¬ íŒ¨í„´
+    if any(pattern in error_message for pattern in [
+        "rate limit", "too many requests", "429", "quota exceeded"
+    ]):
+        raise RateLimitError(str(e)) from e
+    
+    # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ íŒ¨í„´
+    elif any(pattern in error_message for pattern in [
+        "connection", "timeout", "network", "dns", "refused"
+    ]):
+        raise NetworkError(str(e)) from e
+    
+    # ì„œë²„ ì—ëŸ¬ íŒ¨í„´
+    elif any(pattern in error_message for pattern in [
+        "500", "502", "503", "504", "server error", "internal error"
+    ]):
+        raise ServerError(str(e)) from e
+    
+    # ì¸ì¦ ì—ëŸ¬ íŒ¨í„´
+    elif any(pattern in error_message for pattern in [
+        "401", "403", "unauthorized", "forbidden", "api key", "authentication"
+    ]):
+        raise AuthenticationError(str(e)) from e
+    
+    # ê¸°íƒ€ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
+    else:
+        raise
\ No newline at end of file
